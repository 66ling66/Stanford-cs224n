{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 上下文学习与模型优化学习笔记\n",
    "\n",
    "## 目录\n",
    "1. [Zero-Shot和Few-Shot上下文学习](#1-zero-shot和few-shot上下文学习)\n",
    "2. [指令微调](#2-指令微调)\n",
    "3. [人类偏好优化(DPO/RLHF)](#3-人类偏好优化)\n",
    "4. [总结与展望](#4-总结与展望)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Zero-Shot和Few-Shot上下文学习\n",
    "\n",
    "###  上下文学习概述\n",
    "\n",
    "**上下文学习(In-Context Learning, ICL)** 是大型语言模型的一种新兴能力，指模型能够在推理时通过上下文中的示例学习新任务，而无需更新参数。\n",
    "\n",
    "#### 核心特点\n",
    "- **无参数更新**：不需要梯度下降或反向传播\n",
    "- **即时适应**：通过输入格式即可适应新任务\n",
    "- **灵活性强**：同一模型可以处理多种不同任务\n",
    "- **样本高效**：只需要少量示例就能理解任务\n",
    "\n",
    "###  Zero-Shot学习\n",
    "\n",
    "#### 定义\n",
    "**Zero-Shot学习**是指模型在没有看到任何任务特定示例的情况下，仅通过任务描述就能完成任务。\n",
    "\n",
    "#### 工作机制\n",
    "- **任务描述**：通过自然语言描述任务要求\n",
    "- **格式指导**：明确输入输出格式\n",
    "- **预训练知识**：依赖模型在预训练中学到的知识\n",
    "\n",
    "#### 示例格式\n",
    "```\n",
    "任务：将以下句子翻译成中文\n",
    "输入：Hello, how are you?\n",
    "输出：\n",
    "```\n",
    "\n",
    "\n",
    "###  Few-Shot学习\n",
    "\n",
    "#### 定义\n",
    "**Few-Shot学习**是指模型通过少量（通常1-10个）示例来学习和执行新任务。\n",
    "\n",
    "#### 工作机制\n",
    "- **示例学习**：从输入-输出对中学习模式\n",
    "- **模式识别**：识别任务的输入输出关系\n",
    "- **类比推理**：将学到的模式应用到新输入\n",
    "\n",
    "#### 示例格式\n",
    "```\n",
    "任务：情感分析\n",
    "\n",
    "示例1：\n",
    "输入：这部电影太棒了！\n",
    "输出：积极\n",
    "\n",
    "示例2：\n",
    "输入：服务态度很差。\n",
    "输出：消极\n",
    "\n",
    "现在请分析：\n",
    "输入：今天天气不错。\n",
    "输出：\n",
    "```\n",
    "\n",
    "#### Few-Shot的变体\n",
    "\n",
    "##### One-Shot学习\n",
    "- **定义**：仅使用一个示例\n",
    "- **适用场景**：简单任务或模型能力很强时\n",
    "- **优势**：最小化上下文长度\n",
    "\n",
    "##### Many-Shot学习\n",
    "- **定义**：使用较多示例（10-100个）\n",
    "- **适用场景**：复杂任务或需要更好性能时\n",
    "- **挑战**：上下文长度限制\n",
    "\n",
    "### 上下文学习的理论机制\n",
    "\n",
    "#### 涌现能力假说\n",
    "- **规模效应**：模型规模达到临界点后涌现ICL能力\n",
    "- **数据多样性**：预训练数据的多样性促进泛化\n",
    "- **隐式学习**：模型学会了\"学习算法\"\n",
    "\n",
    "#### 注意力机制的作用\n",
    "- **示例检索**：注意力帮助模型找到相关示例\n",
    "- **模式匹配**：识别输入与示例的相似性\n",
    "- **知识迁移**：将示例中的知识迁移到新输入\n",
    "\n",
    "#### 数学表示\n",
    "给定上下文$C = \\{(x_1, y_1), (x_2, y_2), ..., (x_k, y_k)\\}$和查询$x_q$：\n",
    "\n",
    "$$P(y_q|x_q, C) = \\text{LM}(y_q | [C; x_q])$$\n",
    "\n",
    "其中$[C; x_q]$表示将上下文和查询拼接成的序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 指令微调\n",
    "\n",
    "###  指令微调概述\n",
    "\n",
    "**指令微调(Instruction Tuning)**是一种训练方法，通过在大量指令-响应对上微调预训练语言模型，使其能够更好地理解和遵循人类指令。\n",
    "\n",
    "#### 核心思想\n",
    "- **指令理解**：训练模型理解各种形式的人类指令\n",
    "- **任务泛化**：提高模型在未见过任务上的表现\n",
    "- **对齐优化**：使模型行为更符合人类期望\n",
    "\n",
    "\n",
    "###  指令数据的构建\n",
    "\n",
    "#### 数据来源\n",
    "\n",
    "##### 人工标注\n",
    "- **高质量**：人工编写的指令-响应对\n",
    "- **多样性**：覆盖各种任务类型和指令风格\n",
    "- **成本高**：需要大量人力资源\n",
    "\n",
    "##### 现有数据集转换\n",
    "- **格式转换**：将传统NLP数据集转换为指令格式\n",
    "- **模板化**：使用模板生成指令描述\n",
    "- **规模大**：可以快速获得大量数据\n",
    "\n",
    "##### 模型生成\n",
    "- **Self-Instruct**：使用模型自己生成指令数据\n",
    "- **蒸馏方法**：从强模型蒸馏到弱模型\n",
    "- **质量控制**：需要仔细过滤和验证\n",
    "\n",
    "#### 指令格式设计\n",
    "\n",
    "##### 基本格式\n",
    "```\n",
    "指令：[任务描述]\n",
    "输入：[可选的输入内容]\n",
    "输出：[期望的输出]\n",
    "```\n",
    "\n",
    "##### 对话格式\n",
    "```\n",
    "用户：[用户的问题或请求]\n",
    "助手：[模型的回复]\n",
    "```\n",
    "\n",
    "##### 多轮对话格式\n",
    "```\n",
    "用户：[第一轮用户输入]\n",
    "助手：[第一轮助手回复]\n",
    "用户：[第二轮用户输入]\n",
    "助手：[第二轮助手回复]\n",
    "```\n",
    "\n",
    "###  指令微调的训练方法\n",
    "\n",
    "#### 监督微调(Supervised Fine-tuning, SFT)\n",
    "\n",
    "##### 训练目标\n",
    "最大化指令-响应对的似然概率：\n",
    "$$\\mathcal{L}_{SFT} = -\\sum_{i=1}^{N} \\log P(y_i | x_i, \\theta)$$\n",
    "\n",
    "其中$x_i$是指令，$y_i$是期望响应，$\\theta$是模型参数。\n",
    "\n",
    "##### 训练策略\n",
    "- **全参数微调**：更新所有模型参数\n",
    "- **参数高效微调**：只更新部分参数（LoRA、Adapter等）\n",
    "- **渐进式微调**：逐步增加任务复杂度\n",
    "\n",
    "#### 多任务学习\n",
    "- **任务混合**：同时训练多种不同类型的任务\n",
    "- **任务平衡**：确保不同任务的训练平衡\n",
    "- **负迁移避免**：防止任务间的负面影响\n",
    "\n",
    "#### 课程学习\n",
    "- **难度递增**：从简单任务逐步过渡到复杂任务\n",
    "- **技能积累**：先学习基础技能，再学习高级技能\n",
    "- **自适应调整**：根据模型表现调整训练顺序\n",
    "\n",
    "### 指令微调的关键技术\n",
    "\n",
    "#### 数据质量控制\n",
    "\n",
    "##### 质量评估\n",
    "- **人工评估**：专家评估指令和响应的质量\n",
    "- **自动评估**：使用模型评估数据质量\n",
    "- **一致性检查**：确保指令和响应的一致性\n",
    "\n",
    "##### 数据清洗\n",
    "- **去重**：移除重复或近似重复的数据\n",
    "- **过滤**：过滤低质量或有害内容\n",
    "- **标准化**：统一数据格式和风格\n",
    "\n",
    "#### 训练稳定性\n",
    "\n",
    "##### 学习率调度\n",
    "- **预热阶段**：逐步增加学习率\n",
    "- **衰减策略**：训练后期降低学习率\n",
    "- **自适应调整**：根据验证集表现调整\n",
    "\n",
    "##### 正则化技术\n",
    "- **权重衰减**：防止过拟合\n",
    "- **Dropout**：增加模型泛化能力\n",
    "- **梯度裁剪**：防止梯度爆炸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 人类偏好优化(DPO/RLHF)\n",
    "\n",
    "###  人类偏好优化概述\n",
    "\n",
    "**人类偏好优化**是指通过人类反馈来优化语言模型，使其生成的内容更符合人类的价值观和偏好。主要包括**强化学习人类反馈(RLHF)**和**直接偏好优化(DPO)**两种方法。\n",
    "\n",
    "#### 核心动机\n",
    "- **对齐问题**：确保AI系统的行为与人类价值观一致\n",
    "- **安全性**：减少有害、偏见或不当内容的生成\n",
    "- **有用性**：提高模型输出的实用性和相关性\n",
    "- **诚实性**：鼓励模型承认不确定性，避免编造信息\n",
    "\n",
    "###  强化学习人类反馈(RLHF)\n",
    "\n",
    "####  RLHF概述\n",
    "\n",
    "**RLHF(Reinforcement Learning from Human Feedback)**是一种通过人类反馈训练奖励模型，然后使用强化学习优化语言模型的方法。\n",
    "\n",
    "####  RLHF的三阶段流程\n",
    "\n",
    "##### 阶段1：监督微调(SFT)\n",
    "- **目标**：在高质量的指令-响应数据上微调预训练模型\n",
    "- **数据**：人工标注的指令遵循数据\n",
    "- **结果**：获得初始的指令遵循模型\n",
    "\n",
    "##### 阶段2：奖励模型训练\n",
    "- **数据收集**：让SFT模型对同一指令生成多个响应\n",
    "- **人类标注**：人类标注者对响应进行排序或评分\n",
    "- **模型训练**：训练奖励模型预测人类偏好\n",
    "\n",
    "**奖励模型训练目标：**\n",
    "$$\\mathcal{L}_{RM} = -\\mathbb{E}_{(x,y_w,y_l) \\sim D} [\\log \\sigma(r_\\phi(x, y_w) - r_\\phi(x, y_l))]$$\n",
    "\n",
    "其中$y_w$是偏好响应，$y_l$是非偏好响应，$r_\\phi$是奖励模型。\n",
    "\n",
    "##### 阶段3：强化学习优化\n",
    "- **策略优化**：使用PPO等算法优化语言模型\n",
    "- **奖励信号**：奖励模型提供奖励信号\n",
    "- **KL惩罚**：防止模型偏离原始分布太远\n",
    "\n",
    "**PPO目标函数：**\n",
    "$$\\mathcal{L}_{PPO} = \\mathbb{E}_{x \\sim D, y \\sim \\pi_\\theta} [r_\\phi(x, y) - \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)}]$$\n",
    "\n",
    "其中$\\pi_\\theta$是当前策略，$\\pi_{ref}$是参考策略，$\\beta$是KL惩罚系数。\n",
    "\n",
    "\n",
    "###  直接偏好优化(DPO)\n",
    "\n",
    "####  DPO概述\n",
    "\n",
    "**DPO(Direct Preference Optimization)**是一种直接从偏好数据优化语言模型的方法，无需训练单独的奖励模型或使用强化学习。\n",
    "\n",
    "####  DPO的理论基础\n",
    "\n",
    "DPO基于一个关键洞察：最优策略可以通过奖励模型和参考策略的关系来表达：\n",
    "\n",
    "$$\\pi^*(y|x) = \\frac{1}{Z(x)} \\pi_{ref}(y|x) \\exp\\left(\\frac{r(x,y)}{\\beta}\\right)$$\n",
    "\n",
    "通过重新参数化，可以将奖励表示为策略的函数：\n",
    "\n",
    "$$r(x,y) = \\beta \\log \\frac{\\pi(y|x)}{\\pi_{ref}(y|x)} + \\beta \\log Z(x)$$\n",
    "\n",
    "#### DPO训练目标\n",
    "\n",
    "将奖励模型的表达式代入Bradley-Terry模型，得到DPO的损失函数：\n",
    "\n",
    "$$\\mathcal{L}_{DPO} = -\\mathbb{E}_{(x,y_w,y_l) \\sim D} \\left[\\log \\sigma \\left(\\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\right)\\right]$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
