{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN学习笔记\n",
    "## 目录\n",
    "1. [什么是语言模型？](#1-什么是语言模型)\n",
    "2. [N-gram语言模型](#2-n-gram语言模型)\n",
    "3. [固定窗口神经语言模型](#3-固定窗口神经语言模型)\n",
    "4. [循环神经网络(RNN)语言模型](#4-循环神经网络rnn语言模型)\n",
    "5. [代码实现](#5-代码实现)\n",
    "6. [模型对比](#6-模型对比)\n",
    "7. [实际应用](#7-实际应用)\n",
    "8. [练习题](#8-练习题)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 什么是语言模型？\n",
    "\n",
    "###  定义\n",
    "**语言模型（Language Model）** 是一个概率分布，用于计算一个句子或词序列出现的概率。\n",
    "\n",
    "### 数学表示\n",
    "给定一个词序列 $w_1, w_2, ..., w_T$，语言模型计算：\n",
    "\n",
    "$$P(w_1, w_2, ..., w_T) = \\prod_{t=1}^{T} P(w_t | w_1, ..., w_{t-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. N-gram语言模型\n",
    "\n",
    "### 基本思想\n",
    "N-gram模型基于**马尔可夫假设**：当前词只依赖于前面的n-1个词。\n",
    "\n",
    "### 2.2 数学原理\n",
    "\n",
    "#### Unigram (1-gram)\n",
    "$$P(w_1, w_2, ..., w_T) = \\prod_{t=1}^{T} P(w_t)$$\n",
    "\n",
    "#### Bigram (2-gram)\n",
    "$$P(w_1, w_2, ..., w_T) = \\prod_{t=1}^{T} P(w_t | w_{t-1})$$\n",
    "\n",
    "#### Trigram (3-gram)\n",
    "$$P(w_1, w_2, ..., w_T) = \\prod_{t=1}^{T} P(w_t | w_{t-2}, w_{t-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 固定窗口神经语言模型\n",
    "\n",
    "### 3.1 基本思想\n",
    "使用神经网络来学习词的分布式表示，通过固定大小的上下文窗口预测下一个词。\n",
    "\n",
    "### 3.2 模型架构\n",
    "```\n",
    "输入层：[w_{t-n+1}, ..., w_{t-1}] (n-1个词的one-hot向量)\n",
    "    ↓\n",
    "嵌入层：将one-hot向量映射为稠密向量\n",
    "    ↓\n",
    "拼接层：将所有词向量拼接\n",
    "    ↓\n",
    "隐藏层：全连接层 + 激活函数\n",
    "    ↓\n",
    "输出层：Softmax层，输出词汇表上的概率分布\n",
    "```\n",
    "\n",
    "### 3.3 数学表示\n",
    "\n",
    "#### 输入表示\n",
    "$$x = [e(w_{t-n+1}); e(w_{t-n+2}); ...; e(w_{t-1})]$$\n",
    "\n",
    "其中 $e(w)$ 是词 $w$ 的嵌入向量，$[;]$ 表示向量拼接。\n",
    "\n",
    "#### 隐藏层\n",
    "$$h = \\tanh(W_h x + b_h)$$\n",
    "\n",
    "#### 输出层\n",
    "$$\\hat{y} = \\text{softmax}(W_o h + b_o)$$\n",
    "\n",
    "#### 损失函数\n",
    "$$L = -\\sum_{t} \\log P(w_t | w_{t-n+1}, ..., w_{t-1})$$\n",
    "\n",
    "### 3.4 优缺点\n",
    "\n",
    "#### 优点\n",
    "- **解决稀疏性**：通过词嵌入学习词的相似性\n",
    "- **参数共享**：相同的词在不同位置共享参数\n",
    "- **泛化能力强**：可以处理未见过的词组合\n",
    "\n",
    "#### 缺点\n",
    "- **窗口大小固定**：仍然无法捕获长距离依赖\n",
    "- **参数数量大**：窗口大小为n时，需要$n \\times d$的输入维度\n",
    "- **位置敏感**：不同位置的词使用不同的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 循环神经网络(RNN)语言模型\n",
    "\n",
    "### 4.1 基本思想\n",
    "RNN通过隐藏状态来维护历史信息，理论上可以捕获任意长度的依赖关系。\n",
    "\n",
    "### 4.2 模型架构\n",
    "```\n",
    "h_0 → h_1 → h_2 → ... → h_t → h_{t+1}\n",
    "      ↑     ↑             ↑     ↑\n",
    "     w_1   w_2           w_t   w_{t+1}\n",
    "      ↓     ↓             ↓     ↓\n",
    "     y_1   y_2           y_t   y_{t+1}\n",
    "```\n",
    "\n",
    "### 4.3 数学表示\n",
    "\n",
    "#### 隐藏状态更新\n",
    "$$h_t = \\tanh(W_h h_{t-1} + W_x x_t + b_h)$$\n",
    "\n",
    "#### 输出计算\n",
    "$$y_t = \\text{softmax}(W_y h_t + b_y)$$\n",
    "\n",
    "#### 损失函数\n",
    "$$L = -\\sum_{t=1}^{T} \\log P(w_t | w_1, ..., w_{t-1})$$\n",
    "\n",
    "### 4.4 RNN的问题\n",
    "\n",
    "#### 4.4.1 计算效率问题\n",
    "- **顺序计算**：必须按时间步顺序计算，无法并行化\n",
    "- **计算慢**：对于长序列，计算时间线性增长\n",
    "\n",
    "#### 4.4.2 梯度消失问题\n",
    "- **长距离依赖**：随着序列长度增加，早期信息逐渐丢失\n",
    "- **梯度消失**：反向传播时梯度指数衰减\n",
    "\n",
    "数学解释：\n",
    "$$\\frac{\\partial L}{\\partial h_1} = \\frac{\\partial L}{\\partial h_T} \\prod_{t=2}^{T} \\frac{\\partial h_t}{\\partial h_{t-1}}$$\n",
    "\n",
    "当 $\\|\\frac{\\partial h_t}{\\partial h_{t-1}}\\| < 1$ 时，连乘会导致梯度消失。\n",
    "\n",
    "#### 4.4.3 记忆容量限制\n",
    "- **固定维度**：隐藏状态维度固定，容量有限\n",
    "- **信息覆盖**：新信息会覆盖旧信息\n",
    "\n",
    "### 4.5 改进方案\n",
    "- **LSTM**：通过门控机制解决梯度消失\n",
    "- **GRU**：简化的LSTM变体\n",
    "- **Transformer**：完全摒弃循环结构，使用注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 代码实现\n",
    "\n",
    "### 5.1 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import random\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 N-gram语言模型实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel:\n",
    "    def __init__(self, n=2):\n",
    "        self.n = n\n",
    "        self.ngram_counts = defaultdict(int)\n",
    "        self.context_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "    \n",
    "    def train(self, sentences):\n",
    "        \"\"\"训练N-gram模型\"\"\"\n",
    "        for sentence in sentences:\n",
    "            words = ['<START>'] * (self.n - 1) + sentence.lower().split() + ['<END>']\n",
    "            self.vocab.update(words)\n",
    "            \n",
    "            # 统计n-gram和(n-1)-gram的出现次数\n",
    "            for i in range(len(words) - self.n + 1):\n",
    "                ngram = tuple(words[i:i + self.n])\n",
    "                context = tuple(words[i:i + self.n - 1])\n",
    "                \n",
    "                self.ngram_counts[ngram] += 1\n",
    "                self.context_counts[context] += 1\n",
    "    \n",
    "    def probability(self, word, context):\n",
    "        \"\"\"计算P(word|context)\"\"\"\n",
    "        ngram = context + (word,)\n",
    "        \n",
    "        if self.context_counts[context] == 0:\n",
    "            return 1.0 / len(self.vocab)  # 平滑处理\n",
    "        \n",
    "        # 加1平滑\n",
    "        return (self.ngram_counts[ngram] + 1) / (self.context_counts[context] + len(self.vocab))\n",
    "    \n",
    "    def sentence_probability(self, sentence):\n",
    "        \"\"\"计算句子概率\"\"\"\n",
    "        words = ['<START>'] * (self.n - 1) + sentence.lower().split() + ['<END>']\n",
    "        prob = 1.0\n",
    "        \n",
    "        for i in range(self.n - 1, len(words)):\n",
    "            context = tuple(words[i - self.n + 1:i])\n",
    "            word = words[i]\n",
    "            prob *= self.probability(word, context)\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def perplexity(self, test_sentences):\n",
    "        \"\"\"计算困惑度\"\"\"\n",
    "        total_log_prob = 0\n",
    "        total_words = 0\n",
    "        \n",
    "        for sentence in test_sentences:\n",
    "            words = sentence.lower().split()\n",
    "            total_words += len(words) + 1  # +1 for <END>\n",
    "            prob = self.sentence_probability(sentence)\n",
    "            total_log_prob += math.log(prob)\n",
    "        \n",
    "        return math.exp(-total_log_prob / total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 固定窗口神经语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowNeuralLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, window_size):\n",
    "        super(FixedWindowNeuralLM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # 词嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # 隐藏层\n",
    "        self.hidden = nn.Linear(window_size * embedding_dim, hidden_dim)\n",
    "        \n",
    "        # 输出层\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, window_size]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 词嵌入\n",
    "        embeds = self.embedding(x)  # [batch_size, window_size, embedding_dim]\n",
    "        \n",
    "        # 拼接所有词向量\n",
    "        concat_embeds = embeds.view(batch_size, -1)  # [batch_size, window_size * embedding_dim]\n",
    "        \n",
    "        # 隐藏层\n",
    "        hidden = torch.tanh(self.hidden(concat_embeds))\n",
    "        hidden = self.dropout(hidden)\n",
    "        \n",
    "        # 输出层\n",
    "        output = self.output(hidden)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 RNN语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 词嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # RNN层\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # 输出层\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        # x: [batch_size, seq_len]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 初始化隐藏状态\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        # 词嵌入\n",
    "        embeds = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # RNN前向传播\n",
    "        rnn_out, hidden = self.rnn(embeds, hidden)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Dropout\n",
    "        rnn_out = self.dropout(rnn_out)\n",
    "        \n",
    "        # 输出层\n",
    "        output = self.output(rnn_out)  # [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"初始化隐藏状态\"\"\"\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelDataProcessor:\n",
    "    def __init__(self, min_count=1):\n",
    "        self.min_count = min_count\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab_size = 0\n",
    "    \n",
    "    def build_vocab(self, sentences):\n",
    "        \"\"\"构建词汇表\"\"\"\n",
    "        word_counts = Counter()\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = sentence.lower().split()\n",
    "            word_counts.update(words)\n",
    "        \n",
    "        # 添加特殊标记\n",
    "        vocab = ['<PAD>', '<UNK>', '<START>', '<END>']\n",
    "        \n",
    "        # 添加高频词\n",
    "        for word, count in word_counts.items():\n",
    "            if count >= self.min_count:\n",
    "                vocab.append(word)\n",
    "        \n",
    "        # 构建映射\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        self.vocab_size = len(vocab)\n",
    "        \n",
    "        print(f\"词汇表大小: {self.vocab_size}\")\n",
    "    \n",
    "    def sentence_to_indices(self, sentence):\n",
    "        \"\"\"将句子转换为索引序列\"\"\"\n",
    "        words = ['<START>'] + sentence.lower().split() + ['<END>']\n",
    "        indices = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word in self.word2idx:\n",
    "                indices.append(self.word2idx[word])\n",
    "            else:\n",
    "                indices.append(self.word2idx['<UNK>'])\n",
    "        \n",
    "        return indices\n",
    "    \n",
    "    def create_fixed_window_data(self, sentences, window_size):\n",
    "        \"\"\"为固定窗口模型创建训练数据\"\"\"\n",
    "        X, y = [], []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            indices = self.sentence_to_indices(sentence)\n",
    "            \n",
    "            for i in range(window_size, len(indices)):\n",
    "                context = indices[i-window_size:i]\n",
    "                target = indices[i]\n",
    "                X.append(context)\n",
    "                y.append(target)\n",
    "        \n",
    "        return torch.LongTensor(X), torch.LongTensor(y)\n",
    "    \n",
    "    def create_rnn_data(self, sentences, max_len=20):\n",
    "        \"\"\"为RNN模型创建训练数据\"\"\"\n",
    "        X, y = [], []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            indices = self.sentence_to_indices(sentence)\n",
    "            \n",
    "            if len(indices) > max_len:\n",
    "                indices = indices[:max_len]\n",
    "            \n",
    "            # 输入是除了最后一个词的所有词\n",
    "            # 目标是除了第一个词的所有词\n",
    "            if len(indices) > 1:\n",
    "                X.append(indices[:-1])\n",
    "                y.append(indices[1:])\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例数据\n",
    "sentences = [\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "    \"a quick brown dog runs fast\",\n",
    "    \"the lazy cat sleeps all day\",\n",
    "    \"brown animals are beautiful\",\n",
    "    \"the dog and cat are friends\",\n",
    "    \"quick animals run fast\",\n",
    "    \"the fox is clever and quick\",\n",
    "    \"lazy dogs sleep in the sun\",\n",
    "    \"beautiful cats have soft fur\",\n",
    "    \"fast animals catch their prey\"\n",
    "]\n",
    "\n",
    "# 初始化数据处理器\n",
    "processor = LanguageModelDataProcessor(min_count=1)\n",
    "processor.build_vocab(sentences)\n",
    "\n",
    "print(f\"词汇表: {list(processor.word2idx.keys())[:10]}...\")  # 显示前10个词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试N-gram模型\n",
    "print(\"=== N-gram语言模型 ===\")\n",
    "ngram_model = NGramLanguageModel(n=2)  # Bigram\n",
    "ngram_model.train(sentences)\n",
    "\n",
    "# 测试句子概率\n",
    "test_sentence = \"the quick brown fox\"\n",
    "prob = ngram_model.sentence_probability(test_sentence)\n",
    "print(f\"句子 '{test_sentence}' 的概率: {prob:.6f}\")\n",
    "\n",
    "# 计算困惑度\n",
    "test_sentences = [\"the brown dog runs\", \"quick animals are beautiful\"]\n",
    "perplexity = ngram_model.perplexity(test_sentences)\n",
    "print(f\"测试集困惑度: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试固定窗口神经语言模型\n",
    "print(\"\\n=== 固定窗口神经语言模型 ===\")\n",
    "window_size = 3\n",
    "X_fixed, y_fixed = processor.create_fixed_window_data(sentences, window_size)\n",
    "\n",
    "# 创建模型\n",
    "fixed_model = FixedWindowNeuralLM(\n",
    "    vocab_size=processor.vocab_size,\n",
    "    embedding_dim=50,\n",
    "    hidden_dim=100,\n",
    "    window_size=window_size\n",
    ")\n",
    "\n",
    "# 训练设置\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(fixed_model.parameters(), lr=0.001)\n",
    "\n",
    "# 简单训练\n",
    "fixed_model.train()\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = fixed_model(X_fixed)\n",
    "    loss = criterion(outputs, y_fixed)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(f\"训练数据大小: {len(X_fixed)} 个样本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试RNN语言模型\n",
    "print(\"\\n=== RNN语言模型 ===\")\n",
    "X_rnn, y_rnn = processor.create_rnn_data(sentences, max_len=15)\n",
    "\n",
    "# 创建模型\n",
    "rnn_model = RNNLanguageModel(\n",
    "    vocab_size=processor.vocab_size,\n",
    "    embedding_dim=50,\n",
    "    hidden_dim=100,\n",
    "    num_layers=1\n",
    ")\n",
    "\n",
    "# 训练设置\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# 简单训练（注意：这里为了演示简化了训练过程）\n",
    "rnn_model.train()\n",
    "for epoch in range(30):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(len(X_rnn)):\n",
    "        # 将单个序列转换为batch\n",
    "        x = torch.LongTensor([X_rnn[i]])\n",
    "        y = torch.LongTensor([y_rnn[i]])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = rnn_model(x)\n",
    "        \n",
    "        # 重塑输出以匹配损失函数要求\n",
    "        outputs = outputs.view(-1, processor.vocab_size)\n",
    "        y = y.view(-1)\n",
    "        \n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        avg_loss = total_loss / len(X_rnn)\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(f\"训练序列数量: {len(X_rnn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 模型对比\n",
    "\n",
    "### 6.1 性能对比表\n",
    "\n",
    "| 模型类型 | 优点 | 缺点 | 适用场景 |\n",
    "|---------|------|------|----------|\n",
    "| **N-gram** | 简单快速、易于实现 | 稀疏性问题、无法捕获长距离依赖 | 简单任务、基线模型 |\n",
    "| **固定窗口神经LM** | 解决稀疏性、参数共享 | 窗口大小固定、参数量大 | 中等复杂度任务 |\n",
    "| **RNN** | 理论上可捕获任意长依赖 | 梯度消失、计算慢、顺序处理 | 序列建模任务 |\n",
    "\n",
    "### 6.2 计算复杂度对比\n",
    "\n",
    "#### 时间复杂度\n",
    "- **N-gram**: $O(1)$ (查表)\n",
    "- **固定窗口**: $O(n \\times d + h \\times V)$ (n: 窗口大小, d: 嵌入维度, h: 隐藏层大小, V: 词汇表大小)\n",
    "- **RNN**: $O(T \\times (h^2 + h \\times V))$ (T: 序列长度)\n",
    "\n",
    "#### 空间复杂度\n",
    "- **N-gram**: $O(V^n)$ (存储所有n-gram)\n",
    "- **固定窗口**: $O(V \\times d + n \\times d \\times h + h \\times V)$\n",
    "- **RNN**: $O(V \\times d + d \\times h + h^2 + h \\times V)$\n",
    "\n",
    "### 6.3 实际应用建议\n",
    "\n",
    "1. **资源受限环境**: 使用N-gram模型\n",
    "2. **中等规模任务**: 使用固定窗口神经语言模型\n",
    "3. **需要长距离依赖**: 使用LSTM/GRU\n",
    "4. **大规模应用**: 使用Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 实际应用\n",
    "\n",
    "### 7.1 文本生成示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_ngram(model, start_words, max_length=10):\n",
    "    \"\"\"使用N-gram模型生成文本\"\"\"\n",
    "    words = start_words.lower().split()\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # 获取上下文\n",
    "        context = tuple(words[-(model.n-1):])\n",
    "        \n",
    "        # 找到所有可能的下一个词\n",
    "        candidates = []\n",
    "        for ngram in model.ngram_counts:\n",
    "            if ngram[:-1] == context:\n",
    "                candidates.append((ngram[-1], model.ngram_counts[ngram]))\n",
    "        \n",
    "        if not candidates:\n",
    "            break\n",
    "        \n",
    "        # 按概率选择下一个词\n",
    "        total_count = sum(count for _, count in candidates)\n",
    "        probabilities = [count / total_count for _, count in candidates]\n",
    "        \n",
    "        next_word = np.random.choice(\n",
    "            [word for word, _ in candidates],\n",
    "            p=probabilities\n",
    "        )\n",
    "        \n",
    "        if next_word == '<END>':\n",
    "            break\n",
    "        \n",
    "        words.append(next_word)\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# 生成文本示例\n",
    "print(\"=== 文本生成示例 ===\")\n",
    "generated_text = generate_text_ngram(ngram_model, \"the quick\", max_length=8)\n",
    "print(f\"生成的文本: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 困惑度可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较不同n值的N-gram模型性能\n",
    "n_values = [1, 2, 3, 4]\n",
    "perplexities = []\n",
    "\n",
    "test_sentences = [\"the brown dog runs\", \"quick animals are beautiful\", \"lazy cats sleep\"]\n",
    "\n",
    "for n in n_values:\n",
    "    model = NGramLanguageModel(n=n)\n",
    "    model.train(sentences)\n",
    "    perplexity = model.perplexity(test_sentences)\n",
    "    perplexities.append(perplexity)\n",
    "    print(f\"{n}-gram模型困惑度: {perplexity:.2f}\")\n",
    "\n",
    "# 绘制困惑度图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_values, perplexities, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('N-gram大小')\n",
    "plt.ylabel('困惑度')\n",
    "plt.title('不同N-gram模型的困惑度比较')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(n_values)\n",
    "\n",
    "for i, (n, perp) in enumerate(zip(n_values, perplexities)):\n",
    "    plt.annotate(f'{perp:.1f}', (n, perp), textcoords=\"offset points\", \n",
    "                xytext=(0,10), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 练习题\n",
    "\n",
    "### 理论题\n",
    "\n",
    "1. **基础概念**\n",
    "   - 解释语言模型的定义和作用\n",
    "   - 什么是困惑度？如何计算？\n",
    "   - 马尔可夫假设在N-gram模型中的作用是什么？\n",
    "\n",
    "2. **数学推导**\n",
    "   - 推导Bigram模型的最大似然估计公式\n",
    "   - 解释为什么需要平滑技术\n",
    "   - 推导RNN语言模型的梯度计算公式\n",
    "\n",
    "3. **模型对比**\n",
    "   - 比较N-gram和神经语言模型的优缺点\n",
    "   - 为什么RNN会出现梯度消失问题？\n",
    "   - 固定窗口神经语言模型相比N-gram的改进在哪里？\n",
    "\n",
    "### 实践题\n",
    "\n",
    "1. **代码实现**\n",
    "   - 实现Add-k平滑的N-gram模型\n",
    "   - 修改RNN模型，使用LSTM替代普通RNN\n",
    "   - 实现一个简单的文本生成器\n",
    "\n",
    "2. **实验设计**\n",
    "   - 在更大的数据集上比较不同模型的性能\n",
    "   - 分析窗口大小对固定窗口模型性能的影响\n",
    "   - 实现并比较不同的平滑技术\n",
    "\n",
    "3. **应用拓展**\n",
    "   - 将语言模型应用到拼写检查任务\n",
    "   - 实现一个简单的机器翻译评估器\n",
    "   - 设计一个基于语言模型的文本分类器\n",
    "\n",
    "### 思考题\n",
    "\n",
    "1. **深度思考**\n",
    "   - 为什么Transformer能够取代RNN成为主流？\n",
    "   - 如何解决语言模型中的数据稀疏问题？\n",
    "   - 语言模型在多语言场景下面临哪些挑战？\n",
    "\n",
    "2. **前沿探索**\n",
    "   - 了解GPT、BERT等预训练语言模型的基本原理\n",
    "   - 思考语言模型在对话系统中的应用\n",
    "   - 探讨语言模型的伦理和安全问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本笔记详细介绍了语言模型的发展历程，从传统的N-gram模型到神经网络语言模型，再到RNN语言模型。每种模型都有其特点和适用场景：\n",
    "\n",
    "1. **N-gram模型**：简单高效，但存在稀疏性和长距离依赖问题\n",
    "2. **固定窗口神经语言模型**：解决了稀疏性问题，但窗口大小固定\n",
    "3. **RNN语言模型**：理论上可以处理任意长度序列，但存在梯度消失和计算效率问题\n",
    "\n",
    "理解这些模型的原理和局限性，有助于我们更好地理解现代语言模型（如Transformer）的设计思想和优势。\n",
    "\n",
    "### 学习建议\n",
    "1. 从数学原理入手，理解每个模型的核心思想\n",
    "2. 通过代码实现加深对模型的理解\n",
    "3. 在实际数据上测试和比较不同模型的性能\n",
    "4. 关注模型的局限性，思考改进方案\n",
    "5. 了解最新的研究进展和应用场景"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
