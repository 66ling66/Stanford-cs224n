{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN学习笔记\n",
    "## 目录\n",
    "1. [什么是语言模型？](#1-什么是语言模型)\n",
    "2. [N-gram语言模型](#2-n-gram语言模型)\n",
    "3. [固定窗口神经语言模型](#3-固定窗口神经语言模型)\n",
    "4. [循环神经网络(RNN)语言模型](#4-循环神经网络rnn语言模型)\n",
    "5. [代码实现](#5-代码实现)\n",
    "6. [练习题](#8-练习题)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 什么是语言模型？\n",
    "\n",
    "###  定义\n",
    "**语言模型（Language Model）** 是一个概率分布，用于计算一个句子或词序列出现的概率。\n",
    "\n",
    "### 数学表示\n",
    "给定一个词序列 $w_1, w_2, ..., w_T$，语言模型计算：\n",
    "\n",
    "$$P(w_1, w_2, ..., w_T) = \\prod_{t=1}^{T} P(w_t | w_1, ..., w_{t-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. N-gram语言模型\n",
    "\n",
    "### 基本思想\n",
    "N-gram模型基于**马尔可夫假设**：当前词只依赖于前面的n-1个词。\n",
    "\n",
    "### 数学原理\n",
    "\n",
    "#### Unigram (1-gram)\n",
    "$$P(w_1, w_2, ..., w_T) = \\prod_{t=1}^{T} P(w_t)$$\n",
    "\n",
    "#### Bigram (2-gram)\n",
    "$$P(w_1, w_2, ..., w_T) = \\prod_{t=1}^{T} P(w_t | w_{t-1})$$\n",
    "\n",
    "#### Trigram (3-gram)\n",
    "$$P(w_1, w_2, ..., w_T) = \\prod_{t=1}^{T} P(w_t | w_{t-2}, w_{t-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 固定窗口神经语言模型\n",
    "\n",
    "### 3.1 基本思想\n",
    "使用神经网络来学习词的分布式表示，通过固定大小的上下文窗口预测下一个词。\n",
    "\n",
    "### 3.2 模型架构\n",
    "```\n",
    "输入层：[w_{t-n+1}, ..., w_{t-1}] (n-1个词的one-hot向量)\n",
    "    \n",
    "嵌入层：将one-hot向量映射为稠密向量\n",
    "    \n",
    "拼接层：将所有词向量拼接\n",
    "    \n",
    "隐藏层：全连接层 + 激活函数\n",
    "    \n",
    "输出层：Softmax层，输出词汇表上的概率分布\n",
    "```\n",
    "\n",
    "### 3.3 数学表示\n",
    "\n",
    "#### 输入表示\n",
    "$$x = [e(w_{t-n+1}); e(w_{t-n+2}); ...; e(w_{t-1})]$$\n",
    "\n",
    "其中 $e(w)$ 是词 $w$ 的嵌入向量，$[;]$ 表示向量拼接。\n",
    "\n",
    "#### 隐藏层\n",
    "$$h = \\tanh(W_h x + b_h)$$\n",
    "\n",
    "#### 输出层\n",
    "$$\\hat{y} = \\text{softmax}(W_o h + b_o)$$\n",
    "\n",
    "#### 损失函数\n",
    "$$L = -\\sum_{t} \\log P(w_t | w_{t-n+1}, ..., w_{t-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 循环神经网络(RNN)语言模型\n",
    "\n",
    "### 基本思想\n",
    "RNN通过隐藏状态来维护历史信息，理论上可以捕获任意长度的依赖关系。\n",
    "\n",
    "###  模型架构\n",
    "![](img/rnn.png)\n",
    "\n",
    "###  数学表示\n",
    "\n",
    "#### 隐藏状态更新\n",
    "$$h_t = \\tanh(W_h h_{t-1} + W_x x_t + b_h)$$\n",
    "\n",
    "#### 输出计算\n",
    "$$y_t = \\text{softmax}(W_y h_t + b_y)$$\n",
    "\n",
    "#### 损失函数\n",
    "$$L = -\\sum_{t=1}^{T} \\log P(w_t | w_1, ..., w_{t-1})$$\n",
    "\n",
    "###  RNN的问题\n",
    "\n",
    "#### 计算效率问题\n",
    "- **顺序计算**：必须按时间步顺序计算，无法并行化\n",
    "- **计算慢**：对于长序列，计算时间线性增长\n",
    "\n",
    "####  梯度消失问题\n",
    "- **长距离依赖**：随着序列长度增加，早期信息逐渐丢失\n",
    "- **梯度消失**：反向传播时梯度指数衰减"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram语言模型实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel:\n",
    "    def __init__(self, n=2):\n",
    "        self.n = n\n",
    "        self.ngram_counts = defaultdict(int)\n",
    "        self.context_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "    \n",
    "    def train(self, sentences):\n",
    "        \"\"\"训练N-gram模型\"\"\"\n",
    "        for sentence in sentences:\n",
    "            words = ['<START>'] * (self.n - 1) + sentence.lower().split() + ['<END>']\n",
    "            self.vocab.update(words)\n",
    "            \n",
    "            # 统计n-gram和(n-1)-gram的出现次数\n",
    "            for i in range(len(words) - self.n + 1):\n",
    "                ngram = tuple(words[i:i + self.n])\n",
    "                context = tuple(words[i:i + self.n - 1])\n",
    "                \n",
    "                self.ngram_counts[ngram] += 1\n",
    "                self.context_counts[context] += 1\n",
    "    \n",
    "    def probability(self, word, context):\n",
    "        \"\"\"计算P(word|context)\"\"\"\n",
    "        ngram = context + (word,)\n",
    "        \n",
    "        if self.context_counts[context] == 0:\n",
    "            return 1.0 / len(self.vocab)  # 平滑处理\n",
    "        \n",
    "        # 加1平滑\n",
    "        return (self.ngram_counts[ngram] + 1) / (self.context_counts[context] + len(self.vocab))\n",
    "    \n",
    "    def sentence_probability(self, sentence):\n",
    "        \"\"\"计算句子概率\"\"\"\n",
    "        words = ['<START>'] * (self.n - 1) + sentence.lower().split() + ['<END>']\n",
    "        prob = 1.0\n",
    "        \n",
    "        for i in range(self.n - 1, len(words)):\n",
    "            context = tuple(words[i - self.n + 1:i])\n",
    "            word = words[i]\n",
    "            prob *= self.probability(word, context)\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def perplexity(self, test_sentences):\n",
    "        \"\"\"计算困惑度\"\"\"\n",
    "        total_log_prob = 0\n",
    "        total_words = 0\n",
    "        \n",
    "        for sentence in test_sentences:\n",
    "            words = sentence.lower().split()\n",
    "            total_words += len(words) + 1  # +1 for <END>\n",
    "            prob = self.sentence_probability(sentence)\n",
    "            total_log_prob += math.log(prob)\n",
    "        \n",
    "        return math.exp(-total_log_prob / total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 固定窗口神经语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedWindowNeuralLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, window_size):\n",
    "        super(FixedWindowNeuralLM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # 词嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # 隐藏层\n",
    "        self.hidden = nn.Linear(window_size * embedding_dim, hidden_dim)\n",
    "        \n",
    "        # 输出层\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, window_size]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 词嵌入\n",
    "        embeds = self.embedding(x)  # [batch_size, window_size, embedding_dim]\n",
    "        \n",
    "        # 拼接所有词向量\n",
    "        concat_embeds = embeds.view(batch_size, -1)  # [batch_size, window_size * embedding_dim]\n",
    "        \n",
    "        # 隐藏层\n",
    "        hidden = torch.tanh(self.hidden(concat_embeds))\n",
    "        hidden = self.dropout(hidden)\n",
    "        \n",
    "        # 输出层\n",
    "        output = self.output(hidden)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 词嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # RNN层\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # 输出层\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        # x: [batch_size, seq_len]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 初始化隐藏状态\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        # 词嵌入\n",
    "        embeds = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        # RNN前向传播\n",
    "        rnn_out, hidden = self.rnn(embeds, hidden)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Dropout\n",
    "        rnn_out = self.dropout(rnn_out)\n",
    "        \n",
    "        # 输出层\n",
    "        output = self.output(rnn_out)  # [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"初始化隐藏状态\"\"\"\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 练习题(由AI总结应该掌握的知识点）\n",
    "\n",
    "### 理论题\n",
    "\n",
    "1. **基础概念**\n",
    "   - 解释语言模型的定义和作用\n",
    "   - 什么是困惑度？如何计算？\n",
    "   - 马尔可夫假设在N-gram模型中的作用是什么？\n",
    "\n",
    "2. **数学推导**\n",
    "   - 推导Bigram模型的最大似然估计公式\n",
    "   - 解释为什么需要平滑技术\n",
    "   - 推导RNN语言模型的梯度计算公式\n",
    "\n",
    "3. **模型对比**\n",
    "   - 比较N-gram和神经语言模型的优缺点\n",
    "   - 为什么RNN会出现梯度消失问题？\n",
    "   - 固定窗口神经语言模型相比N-gram的改进在哪里？\n",
    "\n",
    "### 实践题\n",
    "\n",
    "1. **代码实现**\n",
    "   - 实现Add-k平滑的N-gram模型\n",
    "   - 修改RNN模型，使用LSTM替代普通RNN\n",
    "   - 实现一个简单的文本生成器\n",
    "\n",
    "2. **实验设计**\n",
    "   - 在更大的数据集上比较不同模型的性能\n",
    "   - 分析窗口大小对固定窗口模型性能的影响\n",
    "   - 实现并比较不同的平滑技术\n",
    "\n",
    "3. **应用拓展**\n",
    "   - 将语言模型应用到拼写检查任务\n",
    "   - 实现一个简单的机器翻译评估器\n",
    "   - 设计一个基于语言模型的文本分类器"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
