{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预训练语言模型学习笔记\n",
    "\n",
    "## 目录\n",
    "1. [Word Structure and Subword Models](#1-word-structure-and-subword-models)\n",
    "2. [Pretraining](#2-pretraining)\n",
    "3. [Fine-tuning](#3-fine-tuning)\n",
    "4. [BERT](#5-bert)\n",
    "5. [T5](#6-t5)\n",
    "6. [练习题](#8-练习题)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word Structure and Subword Models\n",
    "\n",
    "### 传统词汇表示的问题\n",
    "- **词汇表过大**：自然语言中词汇数量庞大\n",
    "- **OOV问题**：Out-of-Vocabulary，未见过的词无法处理\n",
    "- **形态变化**：同一词根的不同形态被视为不同词\n",
    "- **稀有词处理**：低频词缺乏足够训练数据\n",
    "\n",
    "### Subword Models\n",
    "\n",
    "#### Byte Pair Encoding (BPE)\n",
    "- **核心思想**：从字符开始，逐步合并最频繁的字符对\n",
    "- **优势**：平衡词汇表大小和语义完整性\n",
    "\n",
    "#### WordPiece\n",
    "- **Google提出**：用于BERT等模型\n",
    "- **策略**：基于语言模型概率选择合并\n",
    "\n",
    "#### SentencePiece\n",
    "- **语言无关**：不依赖空格分词\n",
    "- **统一处理**：将空格也作为特殊字符处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始tokens: defaultdict(<class 'int'>, {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3})\n",
      "合并 1: ('e', 's') -> {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}\n",
      "合并 2: ('es', 't') -> {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}\n",
      "合并 3: ('est', '</w>') -> {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
      "合并 4: ('l', 'o') -> {'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n",
      "合并 5: ('lo', 'w') -> {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def get_word_tokens(vocab):\n",
    "    \"\"\"将词汇表转换为字符级tokens\"\"\"\n",
    "    tokens = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = ' '.join(list(word)) + ' </w>'\n",
    "        tokens[word_tokens] += freq\n",
    "    return tokens\n",
    "\n",
    "def get_pairs(word_tokens):\n",
    "    \"\"\"获取所有相邻字符对\"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in word_tokens.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, word_tokens):\n",
    "    \"\"\"合并最频繁的字符对\"\"\"\n",
    "    new_word_tokens = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in word_tokens:\n",
    "        new_word = p.sub(''.join(pair), word)\n",
    "        new_word_tokens[new_word] = word_tokens[word]\n",
    "    return new_word_tokens\n",
    "\n",
    "# 示例使用\n",
    "vocab = {'low': 5, 'lower': 2, 'newest': 6, 'widest': 3}\n",
    "word_tokens = get_word_tokens(vocab)\n",
    "print(\"初始tokens:\", word_tokens)\n",
    "\n",
    "# 执行BPE\n",
    "num_merges = 5\n",
    "for i in range(num_merges):\n",
    "    pairs = get_pairs(word_tokens)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best_pair = max(pairs, key=pairs.get)\n",
    "    word_tokens = merge_vocab(best_pair, word_tokens)\n",
    "    print(f\"合并 {i+1}: {best_pair} -> {word_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pretraining\n",
    "\n",
    "###  预训练的核心思想\n",
    "- **无监督学习**：从大量无标注文本中学习语言表示\n",
    "- **通用特征**：学习可迁移的语言特征\n",
    "- **规模效应**：大数据+大模型=强能力\n",
    "\n",
    "###  预训练任务类型\n",
    "\n",
    "#### 语言建模 (Language Modeling)\n",
    "- **目标**：预测下一个词\n",
    "- **公式**：$P(w_t|w_1, w_2, ..., w_{t-1})$\n",
    "\n",
    "#### 掩码语言建模 (Masked Language Modeling)\n",
    "- **BERT使用**：随机掩码15%的词\n",
    "- **双向上下文**：利用前后文信息\n",
    "\n",
    "#### 下一句预测 (Next Sentence Prediction)\n",
    "- **句子关系**：判断两个句子是否连续\n",
    "- **应用**：问答、自然语言推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tuning\n",
    "\n",
    "###  微调策略\n",
    "\n",
    "#### 全参数微调\n",
    "- **更新所有参数**：适用于数据充足的情况\n",
    "- **效果最好**：但计算成本高\n",
    "\n",
    "#### 特征提取\n",
    "- **冻结预训练参数**：只训练任务特定层\n",
    "- **计算高效**：但效果可能受限\n",
    "\n",
    "#### 渐进式微调\n",
    "- **逐层解冻**：从顶层开始逐步解冻\n",
    "- **平衡效果与效率**\n",
    "\n",
    "### 3.2 学习率策略\n",
    "- **较小学习率**：避免破坏预训练知识\n",
    "- **差异化学习率**：不同层使用不同学习率\n",
    "- **Warmup**：逐步增加学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BERT\n",
    "\n",
    "###  BERT核心创新\n",
    "- **双向编码器**：同时利用左右上下文\n",
    "- **Transformer架构**：基于自注意力机制\n",
    "- **预训练+微调**：两阶段训练范式\n",
    "\n",
    "###  BERT架构\n",
    "- **多层Transformer编码器**\n",
    "- **位置编码**：处理序列位置信息\n",
    "- **分段嵌入**：区分不同句子\n",
    "\n",
    "### BERT预训练任务\n",
    "\n",
    "#### Masked Language Model (MLM)\n",
    "- **随机掩码15%的词**\n",
    "- **80%替换为[MASK]，10%随机替换，10%保持不变**\n",
    "\n",
    "#### Next Sentence Prediction (NSP)\n",
    "- **判断两个句子是否连续**\n",
    "- **50%正例，50%负例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT模型使用示例\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertForNextSentencePrediction\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 加载预训练模型\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "mlm_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "nsp_model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# MLM示例\n",
    "def predict_masked_word(text, model, tokenizer):\n",
    "    \"\"\"预测被掩码的词\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = outputs.logits\n",
    "    \n",
    "    # 找到[MASK]位置\n",
    "    mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\n",
    "    \n",
    "    # 获取预测结果\n",
    "    mask_token_logits = predictions[0, mask_token_index, :]\n",
    "    top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "    \n",
    "    print(f\"原句: {text}\")\n",
    "    print(\"Top 5 预测:\")\n",
    "    for token in top_5_tokens:\n",
    "        print(f\"- {tokenizer.decode([token])}\")\n",
    "\n",
    "# NSP示例\n",
    "def predict_next_sentence(sentence_a, sentence_b, model, tokenizer):\n",
    "    \"\"\"预测句子关系\"\"\"\n",
    "    inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = F.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # 0: 不连续, 1: 连续\n",
    "    is_next_prob = predictions[0][0].item()\n",
    "    \n",
    "    print(f\"句子A: {sentence_a}\")\n",
    "    print(f\"句子B: {sentence_b}\")\n",
    "    print(f\"连续概率: {is_next_prob:.4f}\")\n",
    "    print(f\"预测: {'连续' if is_next_prob > 0.5 else '不连续'}\")\n",
    "\n",
    "# 测试MLM\n",
    "predict_masked_word(\"The capital of France is [MASK].\", mlm_model, tokenizer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 测试NSP\n",
    "predict_next_sentence(\n",
    "    \"I went to the store.\", \n",
    "    \"I bought some milk.\", \n",
    "    nsp_model, \n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. T5 (Text-to-Text Transfer Transformer)\n",
    "\n",
    "### T5核心思想\n",
    "- **统一框架**：所有NLP任务都转换为文本到文本\n",
    "- **编码器-解码器架构**：基于Transformer\n",
    "- **任务前缀**：通过前缀指定任务类型\n",
    "\n",
    "### T5架构特点\n",
    "- **相对位置编码**：更好处理长序列\n",
    "- **层归一化**：在每个子层之前\n",
    "- **Gated Linear Unit (GLU)**：在前馈网络中\n",
    "\n",
    "###  T5预训练\n",
    "- **Span Corruption**：掩码连续的文本片段\n",
    "- **去噪自编码器**：重构被破坏的文本\n",
    "\n",
    "###  任务格式化\n",
    "- **翻译**：\"translate English to German: Hello\" → \"Hallo\"\n",
    "- **摘要**：\"summarize: [long text]\" → \"[summary]\"\n",
    "- **问答**：\"question: What is the capital? context: [text]\" → \"Paris\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5模型使用示例\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# 加载T5模型\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "def t5_generate(task_prefix, input_text, model, tokenizer, max_length=50):\n",
    "    \"\"\"使用T5生成文本\"\"\"\n",
    "    # 格式化输入\n",
    "    input_text = f\"{task_prefix}: {input_text}\"\n",
    "    \n",
    "    # 编码\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    \n",
    "    # 生成\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids, \n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    # 解码\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"任务: {task_prefix}\")\n",
    "    print(f\"输入: {input_text}\")\n",
    "    print(f\"输出: {generated_text}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 不同任务示例\n",
    "tasks = [\n",
    "    (\"translate English to German\", \"Hello, how are you?\"),\n",
    "    (\"summarize\", \"The quick brown fox jumps over the lazy dog. This is a common sentence used in typing practice.\"),\n",
    "    (\"cola sentence\", \"The book that I read yesterday was interesting.\"),  # 语法判断\n",
    "]\n",
    "\n",
    "for task_prefix, input_text in tasks:\n",
    "    t5_generate(task_prefix, input_text, model, tokenizer)\n",
    "\n",
    "# Span Corruption预训练任务示例\n",
    "def create_span_corruption_data(text, tokenizer, noise_density=0.15):\n",
    "    \"\"\"创建span corruption训练数据\"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    # 计算要掩码的token数量\n",
    "    num_to_mask = int(len(tokens) * noise_density)\n",
    "    \n",
    "    # 随机选择起始位置\n",
    "    start_idx = torch.randint(0, len(tokens) - num_to_mask + 1, (1,)).item()\n",
    "    \n",
    "    # 创建输入和目标\n",
    "    input_tokens = tokens[:start_idx] + [tokenizer.additional_special_tokens_ids[0]] + tokens[start_idx + num_to_mask:]\n",
    "    target_tokens = [tokenizer.additional_special_tokens_ids[0]] + tokens[start_idx:start_idx + num_to_mask] + [tokenizer.eos_token_id]\n",
    "    \n",
    "    input_text = tokenizer.decode(input_tokens)\n",
    "    target_text = tokenizer.decode(target_tokens)\n",
    "    \n",
    "    print(f\"原文: {text}\")\n",
    "    print(f\"输入: {input_text}\")\n",
    "    print(f\"目标: {target_text}\")\n",
    "\n",
    "# 示例\n",
    "create_span_corruption_data(\"The quick brown fox jumps over the lazy dog\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 练习题\n",
    "\n",
    "### 理论题\n",
    "\n",
    "1. **解释BPE算法的工作原理，并说明其相比于词级别tokenization的优势。**\n",
    "\n",
    "2. **BERT的MLM任务中，为什么要将15%的掩码词中的10%替换为随机词，10%保持不变？**\n",
    "\n",
    "3. **比较BERT和T5的架构差异，说明各自的优缺点。**\n",
    "\n",
    "4. **预训练语言模型存在哪些主要局限性？如何缓解这些问题？**\n",
    "\n",
    "### 编程题\n",
    "\n",
    "1. **实现一个简单的BPE算法**\n",
    "2. **使用BERT进行文本分类任务的微调**\n",
    "3. **实现T5风格的文本到文本任务**\n",
    "4. **分析预训练模型中的偏见**\n",
    "\n",
    "### 思考题\n",
    "\n",
    "1. **如何设计更高效的预训练任务？**\n",
    "2. **预训练模型的规模是否存在上限？**\n",
    "3. **如何平衡模型性能和计算效率？**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
