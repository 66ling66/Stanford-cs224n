{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention与相关技术详解\n",
    "\n",
    "## 目录\n",
    "1. [Self-Attention机制](#1-self-attention机制)\n",
    "2. [Scaled Dot-Product Attention](#2-scaled-dot-product-attention)\n",
    "3. [Multi-Head Self-Attention](#3-multi-head-self-attention)\n",
    "4. [Layer Normalization](#4-layer-normalization)\n",
    "5. [练习题](#6-练习题)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Self-Attention机制\n",
    "\n",
    "### Self-Attention的核心思想\n",
    "\n",
    "**Self-Attention（自注意力）**是Attention机制的一种特殊形式，其中Query、Key和Value都来自同一个输入序列。它允许序列中的每个位置都能关注到序列中的所有位置，包括它自己。\n",
    "\n",
    "### Self-Attention的优势？\n",
    "\n",
    "传统的RNN和CNN在处理序列时存在以下问题：\n",
    "- **RNN**：顺序处理，无法并行化，长距离依赖建模困难\n",
    "- **CNN**：感受野有限，需要多层才能捕获长距离依赖\n",
    "\n",
    "Self-Attention的优势：\n",
    "- **并行计算**：所有位置可以同时计算\n",
    "- **长距离依赖**：任意两个位置的路径长度为1\n",
    "- **动态权重**：根据内容动态分配注意力\n",
    "- **可解释性**：注意力权重提供直观解释\n",
    "\n",
    "###  Self-Attention的数学表示\n",
    "\n",
    "给定输入序列 $X = [x_1, x_2, ..., x_n] \\in \\mathbb{R}^{n \\times d}$：\n",
    "\n",
    "**步骤1：生成Q、K、V矩阵**\n",
    "$$Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V$$\n",
    "\n",
    "其中：\n",
    "- $W^Q \\in \\mathbb{R}^{d \\times d_k}$：查询权重矩阵\n",
    "- $W^K \\in \\mathbb{R}^{d \\times d_k}$：键权重矩阵\n",
    "- $W^V \\in \\mathbb{R}^{d \\times d_v}$：值权重矩阵\n",
    "\n",
    "**步骤2：计算注意力输出**\n",
    "$$\\text{SelfAttention}(X) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\25232\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\25232\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基础Self-Attention实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([2, 10, 256])\n",
      "输出形状: torch.Size([2, 10, 256])\n",
      "注意力权重形状: torch.Size([2, 10, 10])\n",
      "注意力权重和: 1.0000\n"
     ]
    }
   ],
   "source": [
    "class BasicSelfAttention(nn.Module):\n",
    "    \"\"\"基础Self-Attention实现\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_k=None, d_v=None):\n",
    "        super(BasicSelfAttention, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k if d_k is not None else d_model\n",
    "        self.d_v = d_v if d_v is not None else d_model\n",
    "        \n",
    "        # 线性变换层\n",
    "        self.W_q = nn.Linear(d_model, self.d_k, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, self.d_k, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, self.d_v, bias=False)\n",
    "        \n",
    "        # 输出投影\n",
    "        self.W_o = nn.Linear(self.d_v, d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len, d_model] 输入序列\n",
    "            mask: [batch_size, seq_len, seq_len] 注意力掩码\n",
    "        Returns:\n",
    "            output: [batch_size, seq_len, d_model] 输出序列\n",
    "            attention_weights: [batch_size, seq_len, seq_len] 注意力权重\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # 生成Q、K、V\n",
    "        Q = self.W_q(x)  # [batch_size, seq_len, d_k]\n",
    "        K = self.W_k(x)  # [batch_size, seq_len, d_k]\n",
    "        V = self.W_v(x)  # [batch_size, seq_len, d_v]\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        # scores: [batch_size, seq_len, seq_len]\n",
    "        \n",
    "        # 应用掩码（如果提供）\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        # 计算注意力权重\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # 应用注意力权重\n",
    "        context = torch.matmul(attention_weights, V)\n",
    "        # context: [batch_size, seq_len, d_v]\n",
    "        \n",
    "        # 输出投影\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# 测试基础Self-Attention\n",
    "d_model = 256\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "self_attn = BasicSelfAttention(d_model)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output, attn_weights = self_attn(x)\n",
    "\n",
    "print(f\"输入形状: {x.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")\n",
    "print(f\"注意力权重形状: {attn_weights.shape}\")\n",
    "print(f\"注意力权重和: {attn_weights.sum(dim=-1)[0, 0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scaled Dot-Product Attention\n",
    "\n",
    "**Scaled Dot-Product Attention**是Transformer中使用的标准注意力机制。它是最高效的注意力计算方法之一，因为它只涉及矩阵乘法和softmax操作。\n",
    "\n",
    "### 数学公式\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "其中：\n",
    "- $Q \\in \\mathbb{R}^{n \\times d_k}$：查询矩阵\n",
    "- $K \\in \\mathbb{R}^{m \\times d_k}$：键矩阵\n",
    "- $V \\in \\mathbb{R}^{m \\times d_v}$：值矩阵\n",
    "- $\\sqrt{d_k}$：缩放因子\n",
    "\n",
    "### 为什么需要缩放因子？\n",
    "\n",
    "**问题**：当$d_k$很大时，点积$QK^T$的值会变得很大，导致softmax函数进入饱和区域，梯度变得很小。\n",
    "\n",
    "**解决方案**：使用缩放因子$\\frac{1}{\\sqrt{d_k}}$\n",
    "\n",
    "**数学解释**：\n",
    "- 假设$Q$和$K$的元素是独立的随机变量，均值为0，方差为1\n",
    "- 那么$QK^T$中每个元素的方差为$d_k$\n",
    "- 缩放后，方差变为1，保持了合适的数值范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q形状: torch.Size([2, 8, 10, 64])\n",
      "K形状: torch.Size([2, 8, 10, 64])\n",
      "V形状: torch.Size([2, 8, 10, 64])\n",
      "输出形状: torch.Size([2, 8, 10, 64])\n",
      "注意力权重形状: torch.Size([2, 8, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled Dot-Product Attention实现\"\"\"\n",
    "    \n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q: [batch_size, n_heads, seq_len_q, d_k] 查询矩阵\n",
    "            K: [batch_size, n_heads, seq_len_k, d_k] 键矩阵\n",
    "            V: [batch_size, n_heads, seq_len_v, d_v] 值矩阵\n",
    "            mask: [batch_size, n_heads, seq_len_q, seq_len_k] 掩码\n",
    "            temperature: 温度参数，用于控制注意力分布的尖锐程度\n",
    "        Returns:\n",
    "            output: [batch_size, n_heads, seq_len_q, d_v] 输出\n",
    "            attention: [batch_size, n_heads, seq_len_q, seq_len_k] 注意力权重\n",
    "        \"\"\"\n",
    "        d_k = Q.size(-1)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (math.sqrt(d_k) * temperature)\n",
    "        \n",
    "        # 应用掩码\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        # 计算注意力权重\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "        \n",
    "        # 应用注意力权重到值\n",
    "        output = torch.matmul(attention, V)\n",
    "        \n",
    "        return output, attention\n",
    "\n",
    "# 测试Scaled Dot-Product Attention\n",
    "batch_size = 2\n",
    "n_heads = 8\n",
    "seq_len = 10\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "\n",
    "scaled_attn = ScaledDotProductAttention()\n",
    "\n",
    "Q = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "K = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "V = torch.randn(batch_size, n_heads, seq_len, d_v)\n",
    "\n",
    "output, attention = scaled_attn(Q, K, V)\n",
    "\n",
    "print(f\"Q形状: {Q.shape}\")\n",
    "print(f\"K形状: {K.shape}\")\n",
    "print(f\"V形状: {V.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")\n",
    "print(f\"注意力权重形状: {attention.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Head Self-Attention\n",
    "\n",
    "###  Multi-Head Attention的动机\n",
    "\n",
    "单个注意力头可能只能捕获一种类型的依赖关系。Multi-Head Attention通过使用多个\"头\"来\n",
    "\n",
    "\n",
    "###  Multi-Head Attention的数学表示\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "其中每个头计算为：\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "\n",
    "参数矩阵：\n",
    "- $W_i^Q \\in \\mathbb{R}^{d_{model} \\times d_k}$\n",
    "- $W_i^K \\in \\mathbb{R}^{d_{model} \\times d_k}$\n",
    "- $W_i^V \\in \\mathbb{R}^{d_{model} \\times d_v}$\n",
    "- $W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([2, 20, 512])\n",
      "输出形状: torch.Size([2, 20, 512])\n",
      "注意力权重形状: torch.Size([2, 8, 20, 20])\n",
      "参数数量: 1049088\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Self-Attention实现\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % n_heads == 0, \"d_model必须能被n_heads整除\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.d_v = d_model // n_heads\n",
    "        \n",
    "        # 线性变换层\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Scaled Dot-Product Attention\n",
    "        self.attention = ScaledDotProductAttention(dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len, d_model] 输入序列\n",
    "            mask: [batch_size, seq_len, seq_len] 注意力掩码\n",
    "        Returns:\n",
    "            output: [batch_size, seq_len, d_model] 输出序列\n",
    "            attention_weights: [batch_size, n_heads, seq_len, seq_len] 注意力权重\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # 1. 线性变换得到Q, K, V\n",
    "        Q = self.W_q(x)  # [batch_size, seq_len, d_model]\n",
    "        K = self.W_k(x)  # [batch_size, seq_len, d_model]\n",
    "        V = self.W_v(x)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # 2. 重塑为多头格式\n",
    "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.n_heads, self.d_v).transpose(1, 2)\n",
    "        # 现在形状为: [batch_size, n_heads, seq_len, d_k/d_v]\n",
    "        \n",
    "        # 3. 调整掩码维度\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
    "            # [batch_size, n_heads, seq_len, seq_len]\n",
    "            \n",
    "        # 4. 应用Scaled Dot-Product Attention\n",
    "        attn_output, attention_weights = self.attention(Q, K, V, mask)\n",
    "        \n",
    "        # 5. 连接多头输出\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "        \n",
    "        # 6. 最终线性变换\n",
    "        output = self.W_o(attn_output)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# 测试Multi-Head Self-Attention\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "seq_len = 20\n",
    "batch_size = 2\n",
    "\n",
    "mhsa = MultiHeadSelfAttention(d_model, n_heads)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attention_weights = mhsa(x)\n",
    "\n",
    "print(f\"输入形状: {x.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")\n",
    "print(f\"注意力权重形状: {attention_weights.shape}\")\n",
    "print(f\"参数数量: {sum(p.numel() for p in mhsa.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Layer Normalization\n",
    "\n",
    "###  Layer Normalization简介\n",
    "\n",
    "**Layer Normalization（层归一化）**是一种归一化技术，在Transformer架构中起到关键作用。它对每个样本的特征维度进行归一化，而不是像Batch Normalization那样对批次维度进行归一化。\n",
    "\n",
    "### 为什么需要Layer Normalization？\n",
    "\n",
    "1. **稳定训练**：防止梯度爆炸和消失\n",
    "2. **加速收敛**：使优化过程更加稳定\n",
    "3. **减少内部协变量偏移**：减少层间激活分布的变化\n",
    "4. **适合序列模型**：不依赖批次大小，适合变长序列\n",
    "\n",
    "###  数学公式\n",
    "\n",
    "给定输入 $x \\in \\mathbb{R}^{d}$，Layer Normalization计算：\n",
    "\n",
    "$$\\mu = \\frac{1}{d}\\sum_{i=1}^{d} x_i$$\n",
    "\n",
    "$$\\sigma^2 = \\frac{1}{d}\\sum_{i=1}^{d} (x_i - \\mu)^2$$\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "其中：\n",
    "- $\\mu$：均值\n",
    "- $\\sigma^2$：方差\n",
    "- $\\gamma$：可学习的缩放参数\n",
    "- $\\beta$：可学习的偏移参数\n",
    "- $\\epsilon$：数值稳定性常数（通常为$10^{-6}$）\n",
    "- $\\odot$：逐元素乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "归一化前:\n",
      "均值: 4.8818\n",
      "标准差: 9.5532\n",
      "\n",
      "归一化后:\n",
      "均值: -0.0000\n",
      "标准差: 1.0010\n",
      "形状: torch.Size([2, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    \"\"\"Layer Normalization实现\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super(LayerNormalization, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.eps = eps\n",
    "        \n",
    "        # 可学习参数\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len, d_model] 或 [..., d_model]\n",
    "        Returns:\n",
    "            normalized_x: 归一化后的输出，形状与输入相同\n",
    "        \"\"\"\n",
    "        # 计算最后一个维度的均值和方差\n",
    "        mean = x.mean(dim=-1, keepdim=True)  # [..., 1]\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)  # [..., 1]\n",
    "        \n",
    "        # 归一化\n",
    "        normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        \n",
    "        # 缩放和偏移\n",
    "        output = self.gamma * normalized + self.beta\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 测试Layer Normalization\n",
    "d_model = 512\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "layer_norm = LayerNormalization(d_model)\n",
    "x = torch.randn(batch_size, seq_len, d_model) * 10 + 5  # 添加偏移和缩放\n",
    "\n",
    "print(\"归一化前:\")\n",
    "print(f\"均值: {x.mean(dim=-1)[0, 0]:.4f}\")\n",
    "print(f\"标准差: {x.std(dim=-1)[0, 0]:.4f}\")\n",
    "\n",
    "normalized_x = layer_norm(x)\n",
    "\n",
    "print(\"\\n归一化后:\")\n",
    "print(f\"均值: {normalized_x.mean(dim=-1)[0, 0]:.4f}\")\n",
    "print(f\"标准差: {normalized_x.std(dim=-1)[0, 0]:.4f}\")\n",
    "print(f\"形状: {normalized_x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 练习题\n",
    "\n",
    "###  理论练习\n",
    "\n",
    "1. **Self-Attention复杂度分析**：\n",
    "   - 计算Self-Attention的时间复杂度和空间复杂度\n",
    "   - 分析序列长度对计算复杂度的影响\n",
    "   - 比较Self-Attention与RNN的复杂度\n",
    "\n",
    "2. **Multi-Head Attention理解**：\n",
    "   - 解释为什么Multi-Head比单头更有效\n",
    "   - 分析头数对模型性能的影响\n",
    "   - 讨论参数分配策略\n",
    "\n",
    "3. **Layer Normalization分析**：\n",
    "   - 比较Pre-LN和Post-LN的优缺点\n",
    "   - 解释Layer Norm在训练稳定性中的作用\n",
    "   - 分析归一化对梯度流的影响\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
