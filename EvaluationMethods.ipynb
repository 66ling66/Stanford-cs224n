{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然语言处理评估方法笔记\n",
    "\n",
    "## 目录\n",
    "1. [封闭式评估 (Close-ended Evaluations)](#1-封闭式评估)\n",
    "   -  [SuperGLUE](#11-superglue)\n",
    "2. [开放式评估 (Open-ended Evaluations)](#2-开放式评估)\n",
    "   -  [内容重叠度量](#21-内容重叠度量)\n",
    "   -  [基于模型的评估指标](#22-基于模型的评估指标)\n",
    "   -  [人类评估](#23-人类评估)\n",
    "   -  [MMU (Massive Multitask Understanding)](#24-mmu)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 封闭式评估 (Close-ended Evaluations)\n",
    "\n",
    "封闭式评估是指在预定义的数据集上进行的标准化评估，通常有明确的正确答案和评分标准。\n",
    "\n",
    "###  SuperGLUE\n",
    "\n",
    "**定义**: SuperGLUE是一个更具挑战性的语言理解评估基准，是GLUE的升级版本。\n",
    "\n",
    "#### 核心特征\n",
    "- 包含更困难的任务\n",
    "- 需要更深层的语言理解\n",
    "- 评估模型的推理能力\n",
    "- 标准化的评估协议\n",
    "\n",
    "#### SuperGLUE任务详解\n",
    "\n",
    "| 任务名称 | 描述 |\n",
    "|---------|------|\n",
    "| **BoolQ** | 布尔问答任务，判断问题答案是否为真 |\n",
    "| **CB (CommitmentBank)** | 文本蕴含任务，判断前提和假设的关系 |\n",
    "| **COPA** | 因果推理任务，选择最合理的原因或结果 |\n",
    "| **MultiRC** | 多句阅读理解，从多个候选答案中选择 |\n",
    "| **ReCoRD** | 阅读理解任务，需要从文本中提取实体 |\n",
    "| **RTE** | 识别文本蕴含关系 |\n",
    "| **WiC** | 词汇语义理解，判断同一词在不同语境中的含义 |\n",
    "| **WSC** | Winograd模式挑战，代词消歧任务 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 开放式评估 (Open-ended Evaluations)\n",
    "\n",
    "开放式评估用于评估模型在生成任务中的表现，通常没有唯一的标准答案。\n",
    "\n",
    "###  内容重叠度量\n",
    "\n",
    "#### BLEU (Bilingual Evaluation Understudy)\n",
    "\n",
    "**定义**: 基于n-gram精确度的自动评估指标，主要用于机器翻译。\n",
    "\n",
    "**数学公式**:\n",
    "```\n",
    "BLEU = BP × exp(∑(w_n × log(p_n)))\n",
    "```\n",
    "\n",
    "**核心组件**:\n",
    "- **n-gram精确度 (p_n)**: 计算候选文本与参考文本的n-gram重叠度\n",
    "- **简洁性惩罚 (BP)**: 防止生成过短的文本\n",
    "- **几何平均**: 综合不同n-gram的得分\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.7598\n"
     ]
    }
   ],
   "source": [
    "# BLEU分数计算实现\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def calculate_bleu_score(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)):\n",
    "    \"\"\"\n",
    "    计算BLEU分数\n",
    "    \n",
    "    Args:\n",
    "        reference: 参考文本 (字符串)\n",
    "        candidate: 候选文本 (字符串)\n",
    "        weights: n-gram权重 (元组)\n",
    "    \n",
    "    Returns:\n",
    "        BLEU分数 (浮点数)\n",
    "    \"\"\"\n",
    "    ref_tokens = reference.split()\n",
    "    cand_tokens = candidate.split()\n",
    "    \n",
    "    return sentence_bleu([ref_tokens], cand_tokens, weights=weights)\n",
    "\n",
    "def calculate_corpus_bleu(references, candidates):\n",
    "    \"\"\"\n",
    "    计算语料库级别的BLEU分数\n",
    "    \"\"\"\n",
    "    ref_corpus = [[ref.split()] for ref in references]\n",
    "    cand_corpus = [cand.split() for cand in candidates]\n",
    "    \n",
    "    return corpus_bleu(ref_corpus, cand_corpus)\n",
    "\n",
    "# 使用示例\n",
    "reference = \"The cat is on the mat\"\n",
    "candidate = \"A cat is on the mat\"\n",
    "bleu_score = calculate_bleu_score(reference, candidate)\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "\n",
    "**定义**: 基于召回率的评估指标，主要用于文本摘要任务。\n",
    "\n",
    "**数学公式**:\n",
    "```\n",
    "ROUGE-N = ∑Count_match(gram_n) / ∑Count(gram_n)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores:\n",
      "ROUGE-1: P=0.7778, R=0.7778, F1=0.7778\n",
      "ROUGE-2: P=0.6250, R=0.6250, F1=0.6250\n",
      "ROUGE-L: P=0.7778, R=0.7778, F1=0.7778\n"
     ]
    }
   ],
   "source": [
    "# ROUGE分数计算实现\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "\n",
    "def calculate_rouge_scores(reference, candidate):\n",
    "    \"\"\"\n",
    "    计算ROUGE分数\n",
    "    \n",
    "    Args:\n",
    "        reference: 参考文本\n",
    "        candidate: 候选文本\n",
    "    \n",
    "    Returns:\n",
    "        ROUGE分数字典\n",
    "    \"\"\"\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(candidate, reference)[0]\n",
    "    \n",
    "    return {\n",
    "        'rouge-1': {\n",
    "            'precision': scores['rouge-1']['p'],\n",
    "            'recall': scores['rouge-1']['r'],\n",
    "            'f1': scores['rouge-1']['f']\n",
    "        },\n",
    "        'rouge-2': {\n",
    "            'precision': scores['rouge-2']['p'],\n",
    "            'recall': scores['rouge-2']['r'],\n",
    "            'f1': scores['rouge-2']['f']\n",
    "        },\n",
    "        'rouge-l': {\n",
    "            'precision': scores['rouge-l']['p'],\n",
    "            'recall': scores['rouge-l']['r'],\n",
    "            'f1': scores['rouge-l']['f']\n",
    "        }\n",
    "    }\n",
    "\n",
    "# 使用示例\n",
    "reference = \"The quick brown fox jumps over the lazy dog\"\n",
    "candidate = \"A quick brown fox jumps over a lazy dog\"\n",
    "rouge_scores = calculate_rouge_scores(reference, candidate)\n",
    "print(\"ROUGE Scores:\")\n",
    "for metric, scores in rouge_scores.items():\n",
    "    print(f\"{metric.upper()}: P={scores['precision']:.4f}, R={scores['recall']:.4f}, F1={scores['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于模型的评估指标\n",
    "\n",
    "使用预训练模型来评估生成文本的质量，能够捕捉更深层的语义信息。\n",
    "\n",
    "#### BERTScore\n",
    "\n",
    "**定义**: 使用BERT嵌入计算参考文本和候选文本之间的相似度。\n",
    "\n",
    "**核心组件**:\n",
    "- **精确度 (Precision)**: 候选文本中有多少内容在参考文本中\n",
    "- **召回率 (Recall)**: 参考文本中有多少内容在候选文本中\n",
    "- **F1分数**: 精确度和召回率的调和平均\n",
    "\n",
    "\n",
    "#### BLEURT\n",
    "\n",
    "**定义**: 基于BERT的学习评估指标，在人类评估数据上进行微调。\n",
    "\n",
    "**特点**:\n",
    "- 端到端训练\n",
    "- 更好的人类评估相关性\n",
    "- 适应多种生成任务\n",
    "\n",
    "#### COMET\n",
    "\n",
    "**定义**: 跨语言优化度量，专门用于机器翻译评估。\n",
    "\n",
    "**变体**:\n",
    "- **COMET-QE**: 无参考评估\n",
    "- **COMET-Rank**: 排序评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 人类评估\n",
    "\n",
    "通过人类专家或众包工作者对模型输出进行主观评估。\n",
    "\n",
    "#### 评估维度\n",
    "\n",
    "| 维度 | 描述 |\n",
    "|------|------|\n",
    "| **流畅性 (Fluency)** | 文本的语法正确性和可读性 |\n",
    "| **准确性 (Accuracy)** | 信息的正确性和事实性 |\n",
    "| **相关性 (Relevance)** | 与输入或任务的相关程度 |\n",
    "| **连贯性 (Coherence)** | 文本内部的逻辑一致性 |\n",
    "| **创造性 (Creativity)** | 内容的新颖性和原创性 |\n",
    "\n",
    "#### 评估方法\n",
    "\n",
    "1. **绝对评分**: 使用Likert量表进行评分 (1-5分或1-7分)\n",
    "2. **相对排序**: 对多个候选输出进行排序比较\n",
    "3. **成对比较**: 两两比较确定优劣关系\n",
    "\n",
    "###  MMU (Massive Multitask Understanding)\n",
    "\n",
    "**定义**: 大规模多任务理解评估，测试模型在多个领域的综合能力。\n",
    "\n",
    "#### 核心特征\n",
    "- 涵盖57个学科领域\n",
    "- 从基础到高级的难度层次\n",
    "- 多选题格式\n",
    "- 标准化评估协议\n",
    "\n",
    "#### 评估领域\n",
    "- **人文学科 (Humanities)**\n",
    "- **社会科学 (Social Sciences)**\n",
    "- **科学技术 (STEM)**\n",
    "- **其他专业领域 (Other)**\n",
    "\n",
    "#### 评估方面\n",
    "- 世界知识\n",
    "- 问题解决能力\n",
    "- 推理能力\n",
    "- 专业知识理解"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
