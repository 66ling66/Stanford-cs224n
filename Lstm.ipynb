{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM学习笔记\n",
    "\n",
    "## 目录\n",
    "1. [LSTM简介](#1-lstm简介)\n",
    "2. [RNN的局限性回顾](#2-rnn的局限性回顾)\n",
    "3. [LSTM的核心思想](#3-lstm的核心思想)\n",
    "4. [LSTM的详细结构](#4-lstm的详细结构)\n",
    "5. [LSTM的数学原理](#5-lstm的数学原理)\n",
    "6. [代码实现](#7-代码实现)\n",
    "7. [练习题](#10-练习题)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LSTM简介\n",
    "\n",
    "**LSTM（Long Short-Term Memory）** 是一种特殊的循环神经网络（RNN），由Hochreiter和Schmidhuber在1997年提出。LSTM专门设计用来解决传统RNN的梯度消失问题，能够学习长期依赖关系。\n",
    "\n",
    "### LSTM的核心优势\n",
    "- **解决梯度消失问题**：通过门控机制控制信息流\n",
    "- **长期记忆能力**：能够记住很久之前的信息\n",
    "- **选择性遗忘**：能够选择性地遗忘不重要的信息\n",
    "- **灵活的信息更新**：可以决定何时更新记忆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RNN的局限性\n",
    "\n",
    "- 梯度消失问题\n",
    "在传统RNN中，梯度在反向传播过程中会指数衰减：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial h_1} = \\frac{\\partial L}{\\partial h_T} \\prod_{t=2}^{T} \\frac{\\partial h_t}{\\partial h_{t-1}}$$\n",
    "\n",
    "当 $\\|\\frac{\\partial h_t}{\\partial h_{t-1}}\\| < 1$ 时，连乘导致梯度消失。\n",
    "\n",
    "- 长期依赖问题\n",
    "\n",
    "- 信息覆盖问题\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM的核心思想\n",
    "\n",
    "###  细胞状态（Cell State）\n",
    "LSTM的关键创新是引入了**细胞状态** $C_t$，它像一条传送带，信息可以在其上流动而几乎不发生变化。\n",
    "\n",
    "### 门控机制（Gate Mechanism）\n",
    "LSTM通过三个门来控制信息流：\n",
    "1. **遗忘门（Forget Gate）**：决定从细胞状态中丢弃什么信息\n",
    "2. **输入门（Input Gate）**：决定什么新信息被存储在细胞状态中\n",
    "3. **输出门（Output Gate）**：决定输出什么部分的细胞状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTM的详细结构\n",
    "\n",
    "###  LSTM单元结构图\n",
    "![LSTM结构图](img/lstm.png)\n",
    "\n",
    "###  符号说明\n",
    "- $h_t$：隐藏状态（输出）\n",
    "- $C_t$：细胞状态\n",
    "- $x_t$：当前输入\n",
    "- $f_t$：遗忘门\n",
    "- $i_t$：输入门\n",
    "- $\\tilde{C}_t$：候选值\n",
    "- $o_t$：输出门\n",
    "- $\\sigma$：sigmoid函数\n",
    "- $\\tanh$：双曲正切函数\n",
    "\n",
    "###  信息流动过程\n",
    "1. **遗忘阶段**：决定丢弃什么信息\n",
    "2. **输入阶段**：决定存储什么新信息\n",
    "3. **更新阶段**：更新细胞状态\n",
    "4. **输出阶段**：决定输出什么信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LSTM的数学原理\n",
    "\n",
    "### 遗忘门（Forget Gate）\n",
    "遗忘门决定从细胞状态中丢弃什么信息：\n",
    "\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "- 输出值在0到1之间\n",
    "- 1表示\"完全保留\"\n",
    "- 0表示\"完全丢弃\"\n",
    "\n",
    "###  输入门（Input Gate）\n",
    "输入门决定什么新信息被存储在细胞状态中：\n",
    "\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "\n",
    "候选值向量：\n",
    "$$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n",
    "\n",
    "###  细胞状态更新\n",
    "结合遗忘门和输入门来更新细胞状态：\n",
    "\n",
    "$$C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$$\n",
    "\n",
    "这个公式是LSTM的核心：\n",
    "- $f_t * C_{t-1}$：保留旧信息\n",
    "- $i_t * \\tilde{C}_t$：添加新信息\n",
    "\n",
    "### 输出门（Output Gate）\n",
    "输出门决定输出什么部分的细胞状态：\n",
    "\n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "\n",
    "隐藏状态（输出）：\n",
    "$$h_t = o_t * \\tanh(C_t)$$\n",
    "\n",
    "###  完整的LSTM前向传播\n",
    "```\n",
    "1. f_t = σ(W_f · [h_{t-1}, x_t] + b_f)     # 遗忘门\n",
    "2. i_t = σ(W_i · [h_{t-1}, x_t] + b_i)     # 输入门\n",
    "3. C̃_t = tanh(W_C · [h_{t-1}, x_t] + b_C)  # 候选值\n",
    "4. C_t = f_t * C_{t-1} + i_t * C̃_t        # 更新细胞状态\n",
    "5. o_t = σ(W_o · [h_{t-1}, x_t] + b_o)     # 输出门\n",
    "6. h_t = o_t * tanh(C_t)                   # 输出隐藏状态\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  手动实现LSTM单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    \"\"\"手动实现的LSTM单元\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 遗忘门参数\n",
    "        self.W_f = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        # 输入门参数\n",
    "        self.W_i = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        # 候选值参数\n",
    "        self.W_C = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        # 输出门参数\n",
    "        self.W_o = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        # 激活函数\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, x, hidden_state):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        x: 输入 [batch_size, input_size]\n",
    "        hidden_state: (h_{t-1}, C_{t-1})\n",
    "        \"\"\"\n",
    "        h_prev, C_prev = hidden_state\n",
    "        \n",
    "        # 拼接输入和前一时刻的隐藏状态\n",
    "        combined = torch.cat([x, h_prev], dim=1)  # [batch_size, input_size + hidden_size]\n",
    "        \n",
    "        # 1. 遗忘门\n",
    "        f_t = self.sigmoid(self.W_f(combined))\n",
    "        \n",
    "        # 2. 输入门\n",
    "        i_t = self.sigmoid(self.W_i(combined))\n",
    "        \n",
    "        # 3. 候选值\n",
    "        C_tilde = self.tanh(self.W_C(combined))\n",
    "        \n",
    "        # 4. 更新细胞状态\n",
    "        C_t = f_t * C_prev + i_t * C_tilde\n",
    "        \n",
    "        # 5. 输出门\n",
    "        o_t = self.sigmoid(self.W_o(combined))\n",
    "        \n",
    "        # 6. 输出隐藏状态\n",
    "        h_t = o_t * self.tanh(C_t)\n",
    "        \n",
    "        return h_t, C_t\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"初始化隐藏状态\"\"\"\n",
    "        h_0 = torch.zeros(batch_size, self.hidden_size)\n",
    "        C_0 = torch.zeros(batch_size, self.hidden_size)\n",
    "        return h_0, C_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 练习题\n",
    "\n",
    "### 理论题\n",
    "\n",
    "1. **基础概念**\n",
    "   - 解释LSTM中每个门的作用\n",
    "   - 为什么LSTM能够解决梯度消失问题？\n",
    "   - 细胞状态和隐藏状态的区别是什么？\n",
    "\n",
    "2. **数学推导**\n",
    "   - 推导LSTM的前向传播公式\n",
    "   - 解释为什么需要sigmoid和tanh激活函数\n",
    "   - 计算LSTM相比RNN增加了多少参数\n",
    "\n",
    "3. **模型对比**\n",
    "   - 比较LSTM和RNN的优缺点\n",
    "   - 什么情况下选择双向LSTM？\n",
    "\n",
    "### 实践题\n",
    "\n",
    "1. **代码实现**\n",
    "   - 实现一个GRU单元\n",
    "   - 修改LSTM模型支持多层和双向\n",
    "   - 实现一个基于LSTM的序列到序列模型\n",
    "\n",
    "2. **实验设计**\n",
    "   - 在长序列数据上比较LSTM和RNN的性能\n",
    "   - 分析不同隐藏层大小对模型性能的影响\n",
    "   - 实现并比较不同的LSTM变体\n",
    "\n",
    "3. **应用拓展**\n",
    "   - 使用LSTM实现一个简单的机器翻译模型\n",
    "   - 构建一个基于LSTM的股价预测模型\n",
    "   - 实现一个LSTM文本分类器\n",
    "\n",
    "### 思考题\n",
    "   - 为什么Transformer能够在很多任务上超越LSTM？\n",
    "   - LSTM在处理什么类型的序列时表现最好？\n",
    "   - 如何解决LSTM的计算效率问题？\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
