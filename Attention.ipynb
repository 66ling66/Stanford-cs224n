{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention机制学习笔记\n",
    "\n",
    "## 目录\n",
    "\n",
    "1. [Attention的数学原理](#2-attention的数学原理)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Attention的数学原理\n",
    "\n",
    "### 基本概念\n",
    "Attention机制可以看作是一个查询-键-值（Query-Key-Value）系统：\n",
    "- **Query (Q)**：查询向量，表示\"我想要什么信息\"\n",
    "- **Key (K)**：键向量，表示\"我有什么信息\"\n",
    "- **Value (V)**：值向量，表示\"具体的信息内容\"\n",
    "\n",
    "### 计算步骤\n",
    "假设我们有一个查询向量 $q$ (query)，和一系列键值对 $(k_1, v_1), (k_2, v_2), ..., (k_n, v_n)$ (key-value pairs)：\n",
    "\n",
    "**步骤1**：计算查询向量与每个键向量的相似度/能量值\n",
    "\n",
    "$$e_i = \\text{score}(q, k_i)$$\n",
    "\n",
    "常见的评分函数有：\n",
    "- 点积：$\\text{score}(q, k) = q^T k$\n",
    "- 缩放点积：$\\text{score}(q, k) = \\frac{q^T k}{\\sqrt{d_k}}$，其中 $d_k$ 是键向量的维度\n",
    "- 加性注意力：$\\text{score}(q, k) = v_a^T \\tanh(W_a q + U_a k)$\n",
    "- 乘性注意力：$\\text{score}(q, k) = q^T W_a k$\n",
    "\n",
    "**步骤2**：使用softmax函数将能量值归一化为注意力权重\n",
    "\n",
    "$$\\alpha_i = \\frac{\\exp(e_i)}{\\sum_{j=1}^{n} \\exp(e_j)}$$\n",
    "\n",
    "**步骤3**：根据注意力权重计算上下文向量\n",
    "\n",
    "$$c = \\sum_{i=1}^{n} \\alpha_i v_i$$\n",
    "\n",
    "这个上下文向量 $c$ 就是Attention的输出，它是值向量的加权和，权重由查询向量和键向量的相似度决定。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
