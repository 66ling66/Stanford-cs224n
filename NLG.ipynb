{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然语言生成(NLG)学习笔记\n",
    "\n",
    "## 目录\n",
    "1. [什么是NLG](#1-什么是nlg)\n",
    "2. [双向编码器自回归解码器](#2-双向编码器自回归解码器)\n",
    "3. [教师强制与学生强制](#3-教师强制与学生强制)\n",
    "4. [Top-k采样](#4-top-k采样)\n",
    "5. [练习题](#7-练习题)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 什么是NLG\n",
    "\n",
    "### NLG定义\n",
    "**自然语言生成(Natural Language Generation, NLG)** 是人工智能的一个分支，专注于让计算机能够产生人类可理解的自然语言文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 双向编码器自回归解码器\n",
    "\n",
    "### 架构概述\n",
    "**双向编码器自回归解码器(Bidirectional Encoder Autoregressive Decoder)** 是现代NLG模型的主流架构，如BART、T5等。\n",
    "\n",
    "###  核心组件\n",
    "\n",
    "#### 双向编码器(Bidirectional Encoder)\n",
    "- **功能**：理解输入序列的完整上下文\n",
    "- **特点**：可以同时看到前后文信息\n",
    "- **实现**：基于Transformer的编码器层\n",
    "\n",
    "#### 自回归解码器(Autoregressive Decoder)\n",
    "- **功能**：逐步生成输出序列\n",
    "- **特点**：只能看到已生成的部分\n",
    "- **实现**：带掩码的Transformer解码器\n",
    "\n",
    "###  工作流程\n",
    "1. **编码阶段**：编码器处理输入序列，生成上下文表示\n",
    "2. **解码阶段**：解码器基于编码器输出和已生成序列，预测下一个词\n",
    "3. **交叉注意力**：解码器通过交叉注意力机制关注编码器输出\n",
    "\n",
    "###  数学表示\n",
    "\n",
    "编码器：\n",
    "$$H = \\text{Encoder}(X) = \\text{BiTransformer}(X)$$\n",
    "\n",
    "解码器：\n",
    "$$P(y_t|y_{<t}, H) = \\text{Decoder}(y_{<t}, H)$$\n",
    "\n",
    "完整生成概率：\n",
    "$$P(Y|X) = \\prod_{t=1}^{T} P(y_t|y_{<t}, H)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([2, 10])\n",
      "目标形状: torch.Size([2, 8])\n",
      "输出形状: torch.Size([2, 8, 10000])\n"
     ]
    }
   ],
   "source": [
    "# 简化的编码器-解码器实现\n",
    "class SimpleEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers):\n",
    "        super(SimpleEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1000, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        # 词嵌入 + 位置编码\n",
    "        embedded = self.embedding(x) + self.pos_encoding[:seq_len]\n",
    "        # 双向编码\n",
    "        encoded = self.transformer(embedded)\n",
    "        return encoded\n",
    "\n",
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers):\n",
    "        super(SimpleDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1000, d_model))\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, tgt, memory, tgt_mask=None):\n",
    "        seq_len = tgt.size(1)\n",
    "        # 词嵌入 + 位置编码\n",
    "        embedded = self.embedding(tgt) + self.pos_encoding[:seq_len]\n",
    "        # 自回归解码\n",
    "        decoded = self.transformer(embedded, memory, tgt_mask=tgt_mask)\n",
    "        # 输出投影\n",
    "        output = self.output_proj(decoded)\n",
    "        return output\n",
    "\n",
    "class EncoderDecoderModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6):\n",
    "        super(EncoderDecoderModel, self).__init__()\n",
    "        self.encoder = SimpleEncoder(vocab_size, d_model, nhead, num_layers)\n",
    "        self.decoder = SimpleDecoder(vocab_size, d_model, nhead, num_layers)\n",
    "        \n",
    "    def forward(self, src, tgt, tgt_mask=None):\n",
    "        # 编码输入\n",
    "        memory = self.encoder(src)\n",
    "        # 解码输出\n",
    "        output = self.decoder(tgt, memory, tgt_mask)\n",
    "        return output\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"生成自回归掩码\"\"\"\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "# 测试模型\n",
    "vocab_size = 10000\n",
    "model = EncoderDecoderModel(vocab_size)\n",
    "\n",
    "# 模拟数据\n",
    "batch_size, src_len, tgt_len = 2, 10, 8\n",
    "src = torch.randint(0, vocab_size, (batch_size, src_len))\n",
    "tgt = torch.randint(0, vocab_size, (batch_size, tgt_len))\n",
    "tgt_mask = model.generate_square_subsequent_mask(tgt_len)\n",
    "\n",
    "output = model(src, tgt, tgt_mask)\n",
    "print(f\"输入形状: {src.shape}\")\n",
    "print(f\"目标形状: {tgt.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 教师强制与学生强制\n",
    "\n",
    "###  教师强制(Teacher Forcing)\n",
    "\n",
    "#### 定义\n",
    "在训练阶段，使用真实的目标序列作为解码器的输入，而不是使用模型自己的预测结果。\n",
    "\n",
    "#### 工作原理\n",
    "- **训练时**：解码器在时间步t使用真实的$y_{t-1}$来预测$y_t$\n",
    "- **优势**：训练稳定，收敛快速\n",
    "- **问题**：训练和推理时的输入分布不一致(Exposure Bias)\n",
    "\n",
    "#### 数学表示\n",
    "训练时：$P(y_t|y_{<t}^{\\text{true}}, X)$\n",
    "\n",
    "推理时：$P(y_t|y_{<t}^{\\text{pred}}, X)$\n",
    "\n",
    "###  学生强制(Student Forcing)\n",
    "\n",
    "#### 定义\n",
    "在训练阶段，使用模型自己的预测结果作为下一步的输入，模拟推理时的情况。\n",
    "\n",
    "#### 工作原理\n",
    "- **训练时**：解码器使用自己预测的$\\hat{y}_{t-1}$来预测$y_t$\n",
    "- **优势**：训练和推理一致\n",
    "- **问题**：训练不稳定，容易发散\n",
    "\n",
    "### 混合策略\n",
    "\n",
    "#### 计划采样(Scheduled Sampling)\n",
    "- **策略**：以概率$p$使用真实标签，以概率$(1-p)$使用模型预测\n",
    "- **动态调整**：训练过程中逐渐减少$p$\n",
    "\n",
    "#### 课程学习(Curriculum Learning)\n",
    "- **从简单到复杂**：先用教师强制，再逐步引入学生强制\n",
    "- **渐进式训练**：平衡稳定性和一致性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "教师强制损失: 7.2767\n",
      "计划采样损失: 37.1256\n"
     ]
    }
   ],
   "source": [
    "# 教师强制 vs 学生强制实现\n",
    "class TeacherStudentTrainer:\n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def teacher_forcing_step(self, src, tgt):\n",
    "        \"\"\"教师强制训练步骤\"\"\"\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # 使用真实目标序列作为输入(除了最后一个token)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        \n",
    "        # 生成掩码\n",
    "        tgt_mask = self.model.generate_square_subsequent_mask(tgt_input.size(1))\n",
    "        \n",
    "        # 前向传播\n",
    "        output = self.model(src, tgt_input, tgt_mask)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = self.criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def student_forcing_step(self, src, tgt, max_len=50):\n",
    "        \"\"\"学生强制训练步骤\"\"\"\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        batch_size = src.size(0)\n",
    "        device = src.device\n",
    "        \n",
    "        # 初始化解码器输入(通常是<BOS>标记)\n",
    "        decoder_input = torch.zeros(batch_size, 1, dtype=torch.long, device=device)\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        # 编码输入\n",
    "        memory = self.model.encoder(src)\n",
    "        \n",
    "        # 逐步生成\n",
    "        for t in range(min(tgt.size(1) - 1, max_len)):\n",
    "            # 解码当前步\n",
    "            tgt_mask = self.model.generate_square_subsequent_mask(decoder_input.size(1))\n",
    "            output = self.model.decoder(decoder_input, memory, tgt_mask)\n",
    "            \n",
    "            # 获取当前步的预测\n",
    "            current_output = output[:, -1, :]\n",
    "            \n",
    "            # 计算损失\n",
    "            target = tgt[:, t + 1]\n",
    "            loss = self.criterion(current_output, target)\n",
    "            total_loss += loss\n",
    "            \n",
    "            # 使用模型预测作为下一步输入\n",
    "            predicted = torch.argmax(current_output, dim=-1, keepdim=True)\n",
    "            decoder_input = torch.cat([decoder_input, predicted], dim=1)\n",
    "        \n",
    "        # 反向传播\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return total_loss.item()\n",
    "    \n",
    "    def scheduled_sampling_step(self, src, tgt, sampling_prob=0.5):\n",
    "        \"\"\"计划采样训练步骤\"\"\"\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        batch_size = src.size(0)\n",
    "        seq_len = tgt.size(1) - 1\n",
    "        device = src.device\n",
    "        \n",
    "        # 编码输入\n",
    "        memory = self.model.encoder(src)\n",
    "        \n",
    "        # 初始化解码器输入\n",
    "        decoder_input = tgt[:, :1]  # 开始标记\n",
    "        total_loss = 0\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # 解码当前步\n",
    "            tgt_mask = self.model.generate_square_subsequent_mask(decoder_input.size(1))\n",
    "            output = self.model.decoder(decoder_input, memory, tgt_mask)\n",
    "            current_output = output[:, -1, :]\n",
    "            \n",
    "            # 计算损失\n",
    "            target = tgt[:, t + 1]\n",
    "            loss = self.criterion(current_output, target)\n",
    "            total_loss += loss\n",
    "            \n",
    "            # 计划采样：随机选择使用真实标签还是预测结果\n",
    "            use_teacher = torch.rand(1).item() < sampling_prob\n",
    "            \n",
    "            if use_teacher:\n",
    "                # 使用真实标签\n",
    "                next_input = tgt[:, t + 1:t + 2]\n",
    "            else:\n",
    "                # 使用模型预测\n",
    "                next_input = torch.argmax(current_output, dim=-1, keepdim=True)\n",
    "            \n",
    "            decoder_input = torch.cat([decoder_input, next_input], dim=1)\n",
    "        \n",
    "        # 反向传播\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return total_loss.item()\n",
    "\n",
    "# 示例使用\n",
    "model = EncoderDecoderModel(vocab_size=1000)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # 忽略padding\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "trainer = TeacherStudentTrainer(model, criterion, optimizer)\n",
    "\n",
    "# 模拟数据\n",
    "src = torch.randint(1, 1000, (4, 10))\n",
    "tgt = torch.randint(1, 1000, (4, 8))\n",
    "\n",
    "# 测试不同训练策略\n",
    "tf_loss = trainer.teacher_forcing_step(src, tgt)\n",
    "print(f\"教师强制损失: {tf_loss:.4f}\")\n",
    "\n",
    "ss_loss = trainer.scheduled_sampling_step(src, tgt, sampling_prob=0.7)\n",
    "print(f\"计划采样损失: {ss_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Top-k采样\n",
    "\n",
    "### 文本生成中的采样策略\n",
    "\n",
    "#### 贪心解码(Greedy Decoding)\n",
    "- **策略**：每步选择概率最高的词\n",
    "- **优点**：确定性，计算简单\n",
    "- **缺点**：容易陷入重复，缺乏多样性\n",
    "\n",
    "#### 束搜索(Beam Search)\n",
    "- **策略**：保持k个最佳候选序列\n",
    "- **优点**：相对最优解\n",
    "- **缺点**：仍然缺乏多样性\n",
    "\n",
    "###  Top-k采样\n",
    "\n",
    "#### 核心思想\n",
    "在每个时间步，只从概率最高的k个词中进行采样，过滤掉低概率的词。\n",
    "\n",
    "#### 算法步骤\n",
    "\n",
    "1. **计算概率分布**：$P(w|context)$\n",
    "2. **选择Top-k**：保留概率最高的k个词\n",
    "3. **重新归一化**：对选中的k个词重新计算概率\n",
    "4. **随机采样**：从重新归一化的分布中采样\n",
    "\n",
    "#### 数学表示\n",
    "$$P_{\\text{top-k}}(w_i) = \\begin{cases}\n",
    "\\frac{P(w_i)}{\\sum_{j \\in \\text{top-k}} P(w_j)} & \\text{if } w_i \\in \\text{top-k} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_sampling(logits):\n",
    "    \"\"\"贪心采样\"\"\"\n",
    "    return torch.argmax(logits, dim=-1)\n",
    "\n",
    "def top_k_sampling(logits, k=50, temperature=1.0):\n",
    "    \"\"\"Top-k采样\"\"\"\n",
    "    # 应用温度\n",
    "    logits = logits / temperature\n",
    "    \n",
    "    # 获取top-k的值和索引\n",
    "    top_k_logits, top_k_indices = torch.topk(logits, k, dim=-1)\n",
    "    \n",
    "    # 创建掩码，将非top-k的位置设为负无穷\n",
    "    logits_masked = torch.full_like(logits, float('-inf'))\n",
    "    logits_masked.scatter_(-1, top_k_indices, top_k_logits)\n",
    "    \n",
    "    # 计算概率并采样\n",
    "    probs = F.softmax(logits_masked, dim=-1)\n",
    "    return torch.multinomial(probs, 1).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 练习题\n",
    "\n",
    "### 理论题\n",
    "\n",
    "1. **解释双向编码器自回归解码器架构的优势，并与纯自回归模型进行比较。**\n",
    "\n",
    "2. **教师强制和学生强制各有什么优缺点？在什么情况下应该使用哪种策略？**\n",
    "\n",
    "3. **Top-k采样和Top-p采样的区别是什么？各自适用于什么场景？**\n",
    "\n",
    "4. **温度参数如何影响文本生成的质量和多样性？**\n",
    "\n",
    "### 编程题\n",
    "\n",
    "1. **实现一个支持多种采样策略的文本生成器**\n",
    "2. **比较不同训练策略对模型性能的影响**\n",
    "3. **实现一个简单的文本摘要系统**\n",
    "4. **设计一个评估生成文本质量的指标**\n",
    "\n",
    "### 思考题\n",
    "\n",
    "1. **如何平衡生成文本的流畅性和多样性？**\n",
    "2. **如何控制生成文本的长度和结构？**\n",
    "3. **如何评估NLG系统的性能？**\n",
    "4. **未来NLG技术的发展方向是什么？**\n",
    "\n",
    "### 实践项目\n",
    "\n",
    "1. **构建一个新闻摘要系统**\n",
    "2. **开发一个创意写作助手**\n",
    "3. **实现一个多轮对话机器人**\n",
    "4. **设计一个代码生成工具**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
