{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vector 学习笔记\n",
    "\n",
    "## 目录\n",
    "1. [Word2Vector 简介](#1-word2vector-简介)\n",
    "2. [核心思想](#2-核心思想)\n",
    "3. [数学原理](#3-数学原理)\n",
    "4. [Skip-gram 模型详解](#4-skip-gram-模型详解)\n",
    "5. [CBOW 模型详解](#5-cbow-模型详解)\n",
    "6. [Skip-gram vs CBOW 对比](#6-skip-gram-vs-cbow-对比)\n",
    "7. [代码实现](#7-代码实现)\n",
    "8. [实际应用示例](#8-实际应用示例)\n",
    "9. [进阶技术](#9-进阶技术)\n",
    "10. [练习题](#10-练习题)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word2Vector 简介\n",
    "\n",
    "Word2Vector是由Google在2013年提出的一种词嵌入技术，能够将词汇映射到高维向量空间中。它的核心思想是：**语义相似的词在向量空间中应该距离较近**。\n",
    "\n",
    "### 主要特点：\n",
    "- **分布式表示**：每个词用稠密的实数向量表示\n",
    "- **语义相似性**：相似词的向量在空间中距离较近\n",
    "- **计算效率**：相比传统方法，训练速度更快\n",
    "- **向量运算**：支持词汇间的向量运算（如：king - man + woman ≈ queen）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 核心思想\n",
    "\n",
    "Word2Vector基于**分布假设**（Distributional Hypothesis）：\n",
    "> \"You shall know a word by the company it keeps\" - J.R. Firth\n",
    "\n",
    "即：**一个词的含义由其上下文决定**。\n",
    "\n",
    "### 基本原理：\n",
    "1. 通过大量文本数据学习词汇的分布式表示\n",
    "2. 利用神经网络预测词汇的上下文关系\n",
    "3. 在训练过程中，语义相似的词会获得相似的向量表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数学原理\n",
    "\n",
    "### 目标函数\n",
    "Word2Vector的目标是最大化以下似然函数：\n",
    "\n",
    "$$\\mathcal{L} = \\prod_{t=1}^{T} \\prod_{-c \\leq j \\leq c, j \\neq 0} p(w_{t+j} | w_t)$$\n",
    "\n",
    "对数似然函数：\n",
    "$$\\log \\mathcal{L} = \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log p(w_{t+j} | w_t)$$\n",
    "\n",
    "其中：\n",
    "- $T$ 是语料库中词的总数\n",
    "- $c$ 是上下文窗口大小\n",
    "- $w_t$ 是第 $t$ 个词\n",
    "- $w_{t+j}$ 是 $w_t$ 的上下文词\n",
    "\n",
    "### Softmax 函数\n",
    "条件概率通过softmax函数计算：\n",
    "\n",
    "$$p(w_O | w_I) = \\frac{\\exp(v_{w_O}^T v_{w_I})}{\\sum_{w=1}^{W} \\exp(v_w^T v_{w_I})}$$\n",
    "\n",
    "其中：\n",
    "- $v_w$ 和 $v_w'$ 分别是词 $w$ 的输入和输出向量表示\n",
    "- $W$ 是词汇表大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Skip-gram 模型详解\n",
    "\n",
    "### 4.1 基本思想\n",
    "Skip-gram模型的核心思想是：**给定中心词，预测其上下文词**。\n",
    "\n",
    "### 4.2 模型架构\n",
    "```\n",
    "输入层 → 隐藏层 → 输出层\n",
    "  ↓        ↓        ↓\n",
    "中心词 → 词向量 → 上下文词概率分布\n",
    "```\n",
    "\n",
    "### 4.3 数学推导\n",
    "\n",
    "#### 目标函数\n",
    "对于Skip-gram，我们要最大化：\n",
    "$$\\mathcal{J} = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log p(w_{t+j} | w_t)$$\n",
    "\n",
    "#### 条件概率\n",
    "$$p(w_{t+j} | w_t) = \\frac{\\exp(u_{w_{t+j}}^T v_{w_t})}{\\sum_{w=1}^{W} \\exp(u_w^T v_{w_t})}$$\n",
    "\n",
    "其中：\n",
    "- $v_{w_t}$ 是中心词 $w_t$ 的输入向量\n",
    "- $u_{w_{t+j}}$ 是上下文词 $w_{t+j}$ 的输出向量\n",
    "\n",
    "#### 梯度计算\n",
    "对于中心词向量的梯度：\n",
    "$$\\frac{\\partial \\log p(w_o | w_c)}{\\partial v_c} = u_o - \\sum_{w=1}^{W} p(w|w_c) u_w$$\n",
    "\n",
    "### 4.4 Skip-gram 特点\n",
    "- **适合大语料库**：在大数据集上表现更好\n",
    "- **对低频词友好**：能更好地学习罕见词的表示\n",
    "- **计算复杂度高**：需要预测多个上下文词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CBOW 模型详解\n",
    "\n",
    "### 5.1 基本思想\n",
    "CBOW（Continuous Bag of Words）模型的核心思想是：**给定上下文词，预测中心词**。\n",
    "\n",
    "### 5.2 模型架构\n",
    "```\n",
    "输入层 → 隐藏层 → 输出层\n",
    "  ↓        ↓        ↓\n",
    "上下文词 → 平均词向量 → 中心词概率分布\n",
    "```\n",
    "\n",
    "### 5.3 数学推导\n",
    "\n",
    "#### 目标函数\n",
    "对于CBOW，我们要最大化：\n",
    "$$\\mathcal{J} = \\frac{1}{T} \\sum_{t=1}^{T} \\log p(w_t | w_{t-m}, ..., w_{t-1}, w_{t+1}, ..., w_{t+m})$$\n",
    "\n",
    "#### 上下文表示\n",
    "CBOW将上下文词向量平均作为输入：\n",
    "$$\\hat{v} = \\frac{1}{2m} \\sum_{j=-m, j \\neq 0}^{m} v_{w_{t+j}}$$\n",
    "\n",
    "#### 条件概率\n",
    "$$p(w_t | Context) = \\frac{\\exp(u_{w_t}^T \\hat{v})}{\\sum_{w=1}^{W} \\exp(u_w^T \\hat{v})}$$\n",
    "\n",
    "#### 梯度计算\n",
    "对于上下文词向量的梯度：\n",
    "$$\\frac{\\partial \\log p(w_c | Context)}{\\partial v_{w_j}} = \\frac{1}{2m} \\left( u_{w_c} - \\sum_{w=1}^{W} p(w|Context) u_w \\right)$$\n",
    "\n",
    "### 5.4 CBOW 特点\n",
    "- **训练速度快**：只需预测一个中心词\n",
    "- **适合小语料库**：在小数据集上表现更稳定\n",
    "- **对高频词友好**：更好地学习常见词的表示\n",
    "- **平滑效果**：通过平均上下文向量，减少噪声影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Skip-gram vs CBOW 对比\n",
    "\n",
    "| 特征 | Skip-gram | CBOW |\n",
    "|------|-----------|------|\n",
    "| **预测方向** | 中心词 → 上下文词 | 上下文词 → 中心词 |\n",
    "| **训练速度** | 较慢 | 较快 |\n",
    "| **数据集大小** | 适合大数据集 | 适合小数据集 |\n",
    "| **低频词处理** | 表现更好 | 表现一般 |\n",
    "| **高频词处理** | 表现一般 | 表现更好 |\n",
    "| **语法信息** | 捕获较少 | 捕获较多 |\n",
    "| **语义信息** | 捕获较多 | 捕获较少 |\n",
    "| **计算复杂度** | O(C × log W) | O(log W) |\n",
    "\n",
    "### 选择建议：\n",
    "- **大语料库 + 关注语义**：选择 Skip-gram\n",
    "- **小语料库 + 关注语法**：选择 CBOW\n",
    "- **计算资源有限**：选择 CBOW\n",
    "- **需要处理罕见词**：选择 Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 代码实现\n",
    "\n",
    "### 7.1 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 数据预处理类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecDataProcessor:\n",
    "    def __init__(self, min_count=1, window_size=2):\n",
    "        self.min_count = min_count\n",
    "        self.window_size = window_size\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.word_counts = Counter()\n",
    "        self.vocab_size = 0\n",
    "    \n",
    "    def build_vocab(self, sentences):\n",
    "        \"\"\"构建词汇表\"\"\"\n",
    "        # 统计词频\n",
    "        for sentence in sentences:\n",
    "            words = sentence.lower().split()\n",
    "            self.word_counts.update(words)\n",
    "        \n",
    "        # 过滤低频词\n",
    "        vocab = [word for word, count in self.word_counts.items() if count >= self.min_count]\n",
    "        \n",
    "        # 构建词汇映射\n",
    "        self.word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
    "        self.vocab_size = len(vocab)\n",
    "        \n",
    "        print(f\"词汇表大小: {self.vocab_size}\")\n",
    "    \n",
    "    def generate_skipgram_data(self, sentences):\n",
    "        \"\"\"生成Skip-gram训练数据\"\"\"\n",
    "        data = []\n",
    "        for sentence in sentences:\n",
    "            words = sentence.lower().split()\n",
    "            for i, center_word in enumerate(words):\n",
    "                if center_word not in self.word2idx:\n",
    "                    continue\n",
    "                \n",
    "                center_idx = self.word2idx[center_word]\n",
    "                \n",
    "                # 获取上下文词\n",
    "                for j in range(max(0, i - self.window_size), \n",
    "                             min(len(words), i + self.window_size + 1)):\n",
    "                    if i != j and words[j] in self.word2idx:\n",
    "                        context_idx = self.word2idx[words[j]]\n",
    "                        data.append((center_idx, context_idx))\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def generate_cbow_data(self, sentences):\n",
    "        \"\"\"生成CBOW训练数据\"\"\"\n",
    "        data = []\n",
    "        for sentence in sentences:\n",
    "            words = sentence.lower().split()\n",
    "            for i, center_word in enumerate(words):\n",
    "                if center_word not in self.word2idx:\n",
    "                    continue\n",
    "                \n",
    "                center_idx = self.word2idx[center_word]\n",
    "                context_indices = []\n",
    "                \n",
    "                # 获取上下文词\n",
    "                for j in range(max(0, i - self.window_size), \n",
    "                             min(len(words), i + self.window_size + 1)):\n",
    "                    if i != j and words[j] in self.word2idx:\n",
    "                        context_indices.append(self.word2idx[words[j]])\n",
    "                \n",
    "                if context_indices:\n",
    "                    data.append((context_indices, center_idx))\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Skip-gram 模型实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # 输入词嵌入层（中心词）\n",
    "        self.in_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 输出词嵌入层（上下文词）\n",
    "        self.out_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # 初始化权重\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"初始化权重\"\"\"\n",
    "        initrange = 0.5 / self.embedding_dim\n",
    "        self.in_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(self, center_word, context_word):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        # 获取中心词向量\n",
    "        center_embed = self.in_embeddings(center_word)  # [batch_size, embedding_dim]\n",
    "        # 获取上下文词向量\n",
    "        context_embed = self.out_embeddings(context_word)  # [batch_size, embedding_dim]\n",
    "        \n",
    "        # 计算点积\n",
    "        score = torch.sum(center_embed * context_embed, dim=1)  # [batch_size]\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        \"\"\"获取词向量\"\"\"\n",
    "        return self.in_embeddings.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 CBOW 模型实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # 输入词嵌入层（上下文词）\n",
    "        self.in_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 输出词嵌入层（中心词）\n",
    "        self.out_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # 初始化权重\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"初始化权重\"\"\"\n",
    "        initrange = 0.5 / self.embedding_dim\n",
    "        self.in_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def forward(self, context_words, center_word):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        # 获取上下文词向量并平均\n",
    "        context_embeds = self.in_embeddings(context_words)  # [batch_size, context_size, embedding_dim]\n",
    "        context_mean = torch.mean(context_embeds, dim=1)    # [batch_size, embedding_dim]\n",
    "        \n",
    "        # 获取中心词向量\n",
    "        center_embed = self.out_embeddings(center_word)     # [batch_size, embedding_dim]\n",
    "        \n",
    "        # 计算点积\n",
    "        score = torch.sum(context_mean * center_embed, dim=1)  # [batch_size]\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        \"\"\"获取词向量\"\"\"\n",
    "        return self.in_embeddings.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_skipgram(model, data, processor, epochs=100, lr=0.01):\n",
    "    \"\"\"训练Skip-gram模型\"\"\"\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for center_idx, context_idx in data:\n",
    "            # 正样本\n",
    "            center_tensor = torch.LongTensor([center_idx])\n",
    "            context_tensor = torch.LongTensor([context_idx])\n",
    "            \n",
    "            # 负采样\n",
    "            neg_samples = np.random.choice(processor.vocab_size, 5, replace=False)\n",
    "            neg_samples = [idx for idx in neg_samples if idx != context_idx]\n",
    "            \n",
    "            # 计算正样本损失\n",
    "            pos_score = model(center_tensor, context_tensor)\n",
    "            pos_loss = criterion(pos_score, torch.ones_like(pos_score))\n",
    "            \n",
    "            # 计算负样本损失\n",
    "            neg_loss = 0\n",
    "            for neg_idx in neg_samples[:4]:  # 使用4个负样本\n",
    "                neg_tensor = torch.LongTensor([neg_idx])\n",
    "                neg_score = model(center_tensor, neg_tensor)\n",
    "                neg_loss += criterion(neg_score, torch.zeros_like(neg_score))\n",
    "            \n",
    "            loss = pos_loss + neg_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(data)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def train_cbow(model, data, processor, epochs=100, lr=0.01):\n",
    "    \"\"\"训练CBOW模型\"\"\"\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for context_indices, center_idx in data:\n",
    "            if len(context_indices) == 0:\n",
    "                continue\n",
    "                \n",
    "            # 填充上下文词到固定长度\n",
    "            max_context_len = 4\n",
    "            if len(context_indices) < max_context_len:\n",
    "                context_indices.extend([0] * (max_context_len - len(context_indices)))\n",
    "            else:\n",
    "                context_indices = context_indices[:max_context_len]\n",
    "            \n",
    "            context_tensor = torch.LongTensor([context_indices])\n",
    "            center_tensor = torch.LongTensor([center_idx])\n",
    "            \n",
    "            # 负采样\n",
    "            neg_samples = np.random.choice(processor.vocab_size, 5, replace=False)\n",
    "            neg_samples = [idx for idx in neg_samples if idx != center_idx]\n",
    "            \n",
    "            # 计算正样本损失\n",
    "            pos_score = model(context_tensor, center_tensor)\n",
    "            pos_loss = criterion(pos_score, torch.ones_like(pos_score))\n",
    "            \n",
    "            # 计算负样本损失\n",
    "            neg_loss = 0\n",
    "            for neg_idx in neg_samples[:4]:  # 使用4个负样本\n",
    "                neg_tensor = torch.LongTensor([neg_idx])\n",
    "                neg_score = model(context_tensor, neg_tensor)\n",
    "                neg_loss += criterion(neg_score, torch.zeros_like(neg_score))\n",
    "            \n",
    "            loss = pos_loss + neg_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(data)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 实际应用示例\n",
    "\n",
    "### 8.1 准备示例数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例语料\n",
    "sentences = [\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "    \"a quick brown dog runs fast\",\n",
    "    \"the lazy cat sleeps all day\",\n",
    "    \"brown animals are beautiful\",\n",
    "    \"the dog and cat are friends\",\n",
    "    \"quick animals run fast\",\n",
    "    \"the fox is clever and quick\",\n",
    "    \"lazy animals sleep a lot\",\n",
    "    \"brown fox and brown dog\",\n",
    "    \"beautiful animals are quick\"\n",
    "]\n",
    "\n",
    "print(\"示例语料:\")\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"{i+1}. {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 训练 Skip-gram 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化数据处理器\n",
    "processor_sg = Word2VecDataProcessor(min_count=1, window_size=2)\n",
    "processor_sg.build_vocab(sentences)\n",
    "\n",
    "# 生成训练数据\n",
    "skipgram_data = processor_sg.generate_skipgram_data(sentences)\n",
    "print(f\"Skip-gram训练样本数: {len(skipgram_data)}\")\n",
    "\n",
    "# 创建模型\n",
    "skipgram_model = SkipGramModel(processor_sg.vocab_size, embedding_dim=50)\n",
    "\n",
    "# 训练模型\n",
    "print(\"\\n开始训练Skip-gram模型...\")\n",
    "sg_losses = train_skipgram(skipgram_model, skipgram_data, processor_sg, epochs=100, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 训练 CBOW 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化数据处理器\n",
    "processor_cbow = Word2VecDataProcessor(min_count=1, window_size=2)\n",
    "processor_cbow.build_vocab(sentences)\n",
    "\n",
    "# 生成训练数据\n",
    "cbow_data = processor_cbow.generate_cbow_data(sentences)\n",
    "print(f\"CBOW训练样本数: {len(cbow_data)}\")\n",
    "\n",
    "# 创建模型\n",
    "cbow_model = CBOWModel(processor_cbow.vocab_size, embedding_dim=50)\n",
    "\n",
    "# 训练模型\n",
    "print(\"\\n开始训练CBOW模型...\")\n",
    "cbow_losses = train_cbow(cbow_model, cbow_data, processor_cbow, epochs=100, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 可视化训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制损失曲线\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(sg_losses, label='Skip-gram', color='blue')\n",
    "plt.title('Skip-gram 训练损失')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(cbow_losses, label='CBOW', color='red')\n",
    "plt.title('CBOW 训练损失')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 对比两个模型的损失\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sg_losses, label='Skip-gram', color='blue', linewidth=2)\n",
    "plt.plot(cbow_losses, label='CBOW', color='red', linewidth=2)\n",
    "plt.title('Skip-gram vs CBOW 训练损失对比')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 词向量可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(embeddings, word2idx, idx2word, title):\n",
    "    \"\"\"可视化词向量\"\"\"\n",
    "    # 使用PCA降维到2D\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # 绘制词向量点\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.7, s=100)\n",
    "    \n",
    "    # 添加词汇标签\n",
    "    for i, word in idx2word.items():\n",
    "        plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]), \n",
    "                    xytext=(5, 5), textcoords='offset points', \n",
    "                    fontsize=12, alpha=0.8)\n",
    "    \n",
    "    plt.title(f'{title} 词向量可视化 (PCA降维)', fontsize=16)\n",
    "    plt.xlabel('第一主成分')\n",
    "    plt.ylabel('第二主成分')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# 获取词向量\n",
    "sg_embeddings = skipgram_model.get_embeddings()\n",
    "cbow_embeddings = cbow_model.get_embeddings()\n",
    "\n",
    "# 可视化Skip-gram词向量\n",
    "visualize_embeddings(sg_embeddings, processor_sg.word2idx, processor_sg.idx2word, \"Skip-gram\")\n",
    "\n",
    "# 可视化CBOW词向量\n",
    "visualize_embeddings(cbow_embeddings, processor_cbow.word2idx, processor_cbow.idx2word, \"CBOW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 词汇相似度分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"计算余弦相似度\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "def find_similar_words(word, embeddings, word2idx, idx2word, top_k=5):\n",
    "    \"\"\"找到最相似的词\"\"\"\n",
    "    if word not in word2idx:\n",
    "        return []\n",
    "    \n",
    "    word_idx = word2idx[word]\n",
    "    word_vec = embeddings[word_idx]\n",
    "    \n",
    "    similarities = []\n",
    "    for idx, other_word in idx2word.items():\n",
    "        if idx != word_idx:\n",
    "            other_vec = embeddings[idx]\n",
    "            sim = cosine_similarity(word_vec, other_vec)\n",
    "            similarities.append((other_word, sim))\n",
    "    \n",
    "    # 按相似度排序\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# 测试词汇相似度\n",
    "test_words = ['quick', 'brown', 'dog', 'lazy']\n",
    "\n",
    "print(\"=== Skip-gram 词汇相似度 ===\")\n",
    "for word in test_words:\n",
    "    similar_words = find_similar_words(word, sg_embeddings, processor_sg.word2idx, processor_sg.idx2word)\n",
    "    print(f\"\\n'{word}' 的相似词:\")\n",
    "    for similar_word, similarity in similar_words:\n",
    "        print(f\"  {similar_word}: {similarity:.4f}\")\n",
    "\n",
    "print(\"\\n=== CBOW 词汇相似度 ===\")\n",
    "for word in test_words:\n",
    "    similar_words = find_similar_words(word, cbow_embeddings, processor_cbow.word2idx, processor_cbow.idx2word)\n",
    "    print(f\"\\n'{word}' 的相似词:\")\n",
    "    for similar_word, similarity in similar_words:\n",
    "        print(f\"  {similar_word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 进阶技术\n",
    "\n",
    "### 9.1 负采样 (Negative Sampling)\n",
    "\n",
    "负采样是Word2Vector中的一个重要优化技术，用于解决softmax计算复杂度过高的问题。\n",
    "\n",
    "#### 基本思想：\n",
    "- 不计算所有词的softmax概率\n",
    "- 只对少数负样本进行计算\n",
    "- 将多分类问题转化为二分类问题\n",
    "\n",
    "#### 数学公式：\n",
    "$$\\log \\sigma(v_{w_O}^T v_{w_I}) + \\sum_{i=1}^{k} \\mathbb{E}_{w_i \\sim P_n(w)} [\\log \\sigma(-v_{w_i}^T v_{w_I})]$$\n",
    "\n",
    "其中：\n",
    "- $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ 是sigmoid函数\n",
    "- $k$ 是负样本数量\n",
    "- $P_n(w)$ 是负采样分布\n",
    "\n",
    "#### 负采样分布：\n",
    "$$P(w_i) = \\frac{f(w_i)^{3/4}}{\\sum_{j=0}^{n} f(w_j)^{3/4}}$$\n",
    "\n",
    "### 9.2 层次化Softmax (Hierarchical Softmax)\n",
    "\n",
    "层次化Softmax使用二叉树结构来减少计算复杂度。\n",
    "\n",
    "#### 特点：\n",
    "- 将词汇表组织成二叉树\n",
    "- 每个叶子节点代表一个词\n",
    "- 计算复杂度从 $O(V)$ 降到 $O(\\log V)$\n",
    "\n",
    "### 9.3 子词信息 (Subword Information)\n",
    "\n",
    "FastText扩展了Word2Vector，加入了子词信息。\n",
    "\n",
    "#### 优势：\n",
    "- 处理未登录词（OOV）\n",
    "- 捕获词汇的形态学信息\n",
    "- 对拼写错误更鲁棒"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 练习题\n",
    "\n",
    "### 理论题\n",
    "\n",
    "1. **基础概念**\n",
    "   - 解释Word2Vector的核心思想\n",
    "   - 什么是分布假设？\n",
    "   - Skip-gram和CBOW的主要区别是什么？\n",
    "\n",
    "2. **数学推导**\n",
    "   - 推导Skip-gram的目标函数\n",
    "   - 解释为什么需要负采样\n",
    "   - 计算softmax函数的梯度\n",
    "\n",
    "3. **模型对比**\n",
    "   - 在什么情况下选择Skip-gram？\n",
    "   - 在什么情况下选择CBOW？\n",
    "   - 比较两种模型的计算复杂度\n",
    "\n",
    "### 实践题\n",
    "\n",
    "1. **代码实现**\n",
    "   - 实现一个简单的负采样函数\n",
    "   - 修改训练函数，加入学习率衰减\n",
    "   - 实现词汇相似度搜索功能\n",
    "\n",
    "2. **实验设计**\n",
    "   - 比较不同窗口大小对模型性能的影响\n",
    "   - 测试不同嵌入维度的效果\n",
    "   - 分析负采样数量对训练效果的影响\n",
    "\n",
    "3. **应用拓展**\n",
    "   - 使用预训练的Word2Vector模型\n",
    "   - 实现词汇类比任务（如：king - man + woman = queen）\n",
    "   - 将Word2Vector应用到文本分类任务中\n",
    "\n",
    "### 思考题\n",
    "\n",
    "1. Word2Vector有哪些局限性？\n",
    "2. 如何处理多义词问题？\n",
    "3. Word2Vector与现代Transformer模型的关系？\n",
    "4. 如何评估词向量的质量？\n",
    "\n",
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "Word2Vector是自然语言处理领域的重要突破，它开启了词嵌入的新时代。通过本笔记，我们深入学习了：\n",
    "\n",
    "1. **理论基础**：分布假设、目标函数、数学推导\n",
    "2. **模型架构**：Skip-gram和CBOW的详细对比\n",
    "3. **实现细节**：从数据预处理到模型训练的完整流程\n",
    "4. **优化技术**：负采样、层次化Softmax等进阶方法\n",
    "5. **实际应用**：词汇相似度、可视化、类比推理\n",
    "\n",
    "Word2Vector为后续的深度学习模型奠定了基础，理解其原理对于掌握现代NLP技术具有重要意义。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
